{"/devops-learn/docs/cloud/aws/":{"data":{"":"","acl#ACL":"Network ACL 是子网的防火墙，默认拒绝所有流量。","aws-全球基础设施#AWS 全球基础设施":"AWS 在全球有 27 个区域，区域是分散在世界各地数据中心的物理位置。每个区域都有多个可用区（Availability Zone），每个可用区都是一个独立的、故障隔离的区域。可用区是区域中的一个或多个数据中心。\n例如区域：\nus-east-1 eu-west-1 ap-southeast-1 各个区域直接通过高速网络连接，延迟低。是 AWS 的骨干网。","cidr#CIDR":"每个 VPC 都需要配置一个 CIDR，例如 10.0.0.0/16。 每个子网都需要配置一个 CIDR，例如 10.0.0.0/24，10.0.1.0/24，10.0.2.0/24。 每个子网的 CIDR 必须在 VPC 的 CIDR 范围内。","security-group#Security Group":"Security Group 是 EC2 实例的防火墙，默认拒绝所有流量。","vpc#VPC":"VPC 是 Virtual Private Cloud 的缩写，虚拟私有云。是在 AWS 上的一个逻辑隔离的虚拟网络。\n在区域中可以创建多个 VPC，一个区域最多 5 个 VPC。每个 VPC 都是一个独立的网络环境，可以在 VPC 中创建子网、路由表、安全组等。\n每个子网选择一个可用区，一个可用区可以创建多个子网。\n然后就可以将 EC2 实例部署到不同的子网中。","vpc-路由器#VPC 路由器":"VPC 路由器负责互联子网并在网关、NAT 网关等组件之间路由流量。\n路由表是用来配置 VPC 路由器的。","为什么不建议使用-aws-默认-vpc#为什么不建议使用 AWS 默认 VPC？":"AWS 默认 VPC 是一个为了方便用户快速开始而设计的“入门级”网络环境。它牺牲了安全性、灵活性和最佳实践来换取便捷性。\n第一是安全风险，它的所有子网默认都是公网的，且容易导致不同项目间缺乏网络隔离。 第二是设计僵化，它的 CIDR 块不可更改，极易导致与其他网络连接时发生 IP 冲突，也无法实现公私子网分离的最佳架构。1 第三是可扩展性差，它在多账户策略中会成为噩梦，因为所有账户的默认 VPC 网段都重叠。 第四是违背 IaC 原则，对于像 Terraform 这样的工具，依赖一个隐式的默认资源会使代码缺乏可移植性和明确性。","创建-vpc#创建 VPC":"创建 VPC 会默认创建一个路由表，路由表中会有一个默认路由，指向 Internet 网关。 还会默认创建一个 Network ACL，Network ACL 是子网的防火墙，默认拒绝所有流量。 还会默认创建一个 Security Group，Security Group 是 EC2 实例的防火墙，默认拒绝所有流量。","可用区#可用区":"可用区可以创建子网，可以是公有子网，也可以是私有子网。每个子网始终都是在一个可用区内的。\n可用区一般是由多个数据中心组成的。冗余电源，网络等，保证高可用。","应该怎么做#应该怎么做？":"为每个工作负载创建自定义 VPC：根据应用的需求，精心设计 IP 地址空间（CIDR block），确保不会与任何需要连接的网络重叠（如公司局域网、其他 VPC）。\n设计分层子网：\n公有子网：路由指向 Internet Gateway，用于放置面向公众的负载均衡器、NAT 网关、Bastion Host（跳板机）。 私有子网：路由指向 NAT Gateway（用于出站互联网访问）或仅指向内部网关，用于放置应用程序服务器、工作队列、缓存服务。 隔离子网（或数据子网）：没有通往互联网的路由，用于放置数据库、数据存储等最敏感的后端服务。 配置安全组和网络 ACL：\n只开放必要的端口和协议。 为每个子网创建自定义的 Security Group，限制入站和出站流量。 配置 Network ACL 以控制子网内流量，例如拒绝来自外部的 ICMP 流量。 启用 VPC 流日志（可选）：\n用于监控和分析 VPC 流量，帮助检测异常活动。 监控和维护：\n定期审查和更新安全组和网络 ACL，确保只允许必要的流量。 监控 VPC 流日志，及时发现和响应安全事件。","网关#网关":"如果要访问外部网络，还需要通过网关。每个 VPC 只有一个网关。负责出口流量和接收进入 VPC 的流量（入口流量）。"},"title":"AWS"},"/devops-learn/docs/linux/01_view_text/":{"data":{"":"cat：显示文件的所有内容。 more：读取文件，但不需要读取整个文件到内存中，支持向下翻页。 less：more 的反义词，支持上下翻页。尽量使用 less 这种不需要读取全部文件的指令，因为在线上执行 cat 是一件非常危险的事情，这可能导致线上服务器资源不足。 head：查看文件开头。 head -5 \u003cfile\u003e：显示文件前 5 行的内容。 head -n 5 和 head -5 是一样的。 tail：查看文件结尾： tail -n 5 \u003cfile\u003e：显示文件尾部 5 行的内容。 -f：同步显示更新内容。 wc：统计文件内容。 wc -l \u003cfile\u003e：统计指定文件中的内容有多少行。 wc -w \u003cfile\u003e：统计指定文件中的单词数。 wc -m \u003cfile\u003e：统计指定文件中的字符数。 默认显示行数和文件，使用 cat a.txt | wc -l 可以只显示 a.txt 文件的行数。"},"title":"查看文件内容"},"/devops-learn/docs/linux/02_tar/":{"data":{"":"Linux 里面打包和压缩是分开的两个命令 tar 和 gzip/bzip2。","tar#tar":"tar -cf \u003c压缩文件\u003e \u003c多个目录或文件\u003e -c --create 打包 -f 指定归档文件。总是用 -f 参数指定文件名，并放在参数集的最后。 tar -xf \u003c压缩文件\u003e -x --extract 解压 tar -xf /tmp/backup.tar -C /root： 把 /tmp/backup.tar 文件还原到 /root 目录下。 -C 解压到指定目录 tar -cf - /etc： - 表示压缩到标准输出。直接输出到标准输出没什么用，需要配合 |： 远程备份：tar -cf - /data | ssh user@host \"cat \u003e backup.tar\"，无临时文件，高效流式传输 流式压缩：tar -cf - /data | gzip \u003e backup.tar.gz，灵活选择压缩工具和参数。 内容检查：tar -cf - /data | tar -t -f -，不解压到磁盘即可查看内容。 计算校验和：tar -cf - /data | md5sum，直接计算归档包的哈希值。 加密：tar -cf - /data | gpg -c \u003e backup.tar.gpg，边打包边加密，提升安全性 tar -c /etc 和 tar -cf - /etc 类似，不同的是没有使用 -f 指定归档文件。一般默认的行为也是输出到标准输出。但是有一些历史版本不是的。这条命令的行为是隐式且可能变化的。 其他常用参数：\n-t --list 列出归档文件中的内容列表。例如 tar -tf t.tar。 -z gzip 压缩，tar.gz 或者 tgz。需要解压缩就加上 -z 参数。例如 tar -czf /tmp/backup.tar.gz /etc。tgz 是 .tar.gz 的简写。 -j bzip2 压缩，后缀 tar.bz2 或者 tbz2。需要解压缩就加上 -j 参数。例如 tar -cjf /tmp/backup.tar.bz2 /etc。tbz2 是 .tar.bz2 的简写。 -v --verbose：显示指令执行过程。 --exclude\t排除不需要打包的文件或目录。 tar 命令默认会打包所有文件，包括隐藏文件（以点 . 开头的文件和目录）。 tar -czvf backup_with_star.tar.gz * 不会打包隐藏文件，因为 shell 会先将 * 扩展为所有非隐藏的文件名。 gzip、bzip2 压缩，gzip 压缩更快，bzip2 压缩比例更高。","应用场景#应用场景":"tar -cf - /etc：- 表示将 tar 压缩包写入到标准输出，而不是写入文件。\n# 将 /etc 目录打包并用 gzip 压缩，保存为当前目录下的 etc-backup.tar.gz tar -czvf etc-backup.tar.gz /etc/ # 使用更现代的 zstd 压缩，速度更快 tar -c --zstd -vf app-backup.tar.zst /var/www/myapp/ # 备份网站目录，但排除日志文件和缓存目录 tar -czvf site-backup.tar.gz \\ --exclude='*.log' \\ --exclude='./cache' \\ /var/www/html/ # 使用文件列表来排除 tar -czvf backup.tar.gz -X exclude-list.txt /data/ # exclude-list.txt 内容： # *.tmp # logs/ # temp/ # 不需要特殊参数，tar 默认会保留权限、所有权和时间戳。 # 但在解压时，如果用普通用户解压，所有权信息会丢失（除非是 root）。 # 备份根目录 / 时要小心使用 --exclude 排除虚拟文件系统。 sudo tar -czvf full-system-backup.tar.gz --exclude=/proc \\ --exclude=/sys --exclude=/dev --exclude=/tmp --exclude=/run \\ --exclude=/mnt --exclude=/media / # 列出 backup.tar.gz 里所有的文件 tar -tzvf backup.tar.gz # 查找压缩包里是否包含某个配置文件 tar -tzvf backup.tar.gz | grep nginx.conf # 将本地目录打包压缩后，直接通过 SSH 传输到远程服务器保存 tar -czv /data/ | ssh user@backup-server \"cat \u003e /backup/server-data-$(date +%F).tar.gz\" # 或者直接在远程服务器上解压 tar -czv /data/ | ssh user@backup-server \"tar -xz -C /remote/backup/dir/\" # 打包过去 7 天内修改过的 .log 文件 find /var/log -name \"*.log\" -mtime -7 -exec tar -rvf weekly-logs.tar {} \\; # 然后再压缩 gzip weekly-logs.tar tar -czv /data/ | ssh user@backup-server \"cat \u003e /backup/server-data-$(date +%F).tar.gz\" 使用 cat \u003e 的原因：\n\u003e 是 shell 的一个操作符，它的作用是将它左边命令的标准输出，重定向到右边的文件。不会主动去读取它自己的标准输入（stdin）。"},"title":"打包压缩"},"/devops-learn/docs/linux/03_text_operation/":{"data":{"":"文本搜索一般会使用正则表达式。","awk#awk":"awk 一般用于对文本内容进行统计，按需要的格式进行输出。一般是作为 sed 的一个补充。awk 可以看成是一种编程语言。\n特别擅长处理结构化文本数据（如日志、CSV、命令输出等）。\nawk 和 sed 的区别：\nawk 用于比较规范的文本处理，用于统计数量并输出指定字段 sed 一般用于把不规范的文本处理为规范的文本 awk 的流程控制：\n输入数据前例程 BEGIN{}，读入数据前执行，做一些预处理操作。 主输入循环 {}，处理读取的每一行。 所有文件读取完成例程 END{}，读取操作完成后执行，做一些数据汇总。 常用的写法是只写主输入循环。\n记录和字段：\n每一行叫做 awk 的记录。 使用空格、制表符分隔开的单词叫做字段。 可以指定分隔的字段。 字段的引用：\n$1 $2 … $n 表示每一个字段，$0 表示当前行，awk '{print $1, $2, $3} filename' $NF 是最后一个字段，和 NF 不一样。 -F 改变字段的分隔符，awk -F ',' '{print $1, $2, $3}' filename，分隔符可以使用正则表达式 系统变量：\nNR 表示当前处理的是第几行。 NF 字段数量，所以最后一个字段内容可以用 $NF 取出，$(NF-1) 代表倒数第二个字段。 FS 字段分隔符，默认是空格和制表符。 RS 行分隔符，用于分割每一行，默认是换行符。 OFS 输出的字段分隔符，用于打印时分隔字段，默认为空格。 ORS 输出行分隔符，用于打印时分隔记录，默认为换行符。 FNR 行数。 常用参数：\n-F：指定输入字段分隔符。这是最常用的参数。 awk -F: '{print $1}' /etc/passwd // 使用冒号 : 作为分隔符 awk -F'[ :]' '{print $2}' file // 使用空格或冒号作为分隔符（正则表达式） -v：定义变量，用于从 Shell 向 awk 脚本传递值。awk -v name=\"Alice\" '{print name, $1}' file.txt -f：从脚本文件中读取 awk 命令，用于复杂的脚本。awk -f script.awk data.txt","grep#grep":"grep 用来查找文件里符合条件的字符串。 grep 会把符合条件的行显示出来。\n[root@pooky init.d]# grep password /root/anaconda-ks.cfg # Root password [root@pooky init.d]# grep -i password /root/anaconda-ks.cfg # -i 忽略大小写 # Root password [root@pooky ~]# grep pass.... /root/anaconda-ks.cfg # 可以使用元字符 . 匹配任意一个字符 auth --enableshadow --passalgo=sha512 # Root password [root@pooky ~]# grep pass....$ /root/anaconda-ks.cfg # $ 表示结尾 auth --enableshadow --passalgo=sha512 # Root password [root@pooky ~]# grep pass...d$ /root/anaconda-ks.cfg # Root password [root@pooky ~]# grep pass.*$ /root/anaconda-ks.cfg # .* 就表示任意个字符 auth --enableshadow --passalgo=sha512 # Root password user --groups=wheel --name=admin --passwd=$6$Lh0jvsS/YklFVYDM$WjPFI.WaMd3be/qiyFVUQkjEFN0PGQcnRTJFUDejJMUS24DA.M2rJ039hi/ubRiaNY4QNt661FARlxZqL.nCs0 --iscrypted --gecos=\"admin\" [root@pooky ~]# grep ^# /root/anaconda-ks.cfg # 以 # 为开头的行 #version=DEVEL # System authorization information # Use CDROM installation media # Use graphical install","sed#sed":"sed 命令一般用于对文本内容做替换：sed [-hnV][-e","sort#sort":"对文本行进行排序，几乎总是作为数据管道（pipe）的关键一环，与 uniq, awk, head 等命令组合，用于日志分析、数据统计和报告生成。\n常用参数：\n-n：按数字大小进行排序，而不是字母顺序。 -r --reverse：逆序排序（降序）。 -k：指定排序的键。 -t --field-separator=SEP 指定字段分隔符，默认是空格。处理 CSV 文件或以特定字符（如 :, ,）分隔的日志。 -u --unique：在排序的同时去除重复行。 -o --output=FILE：将结果输出到指定文件，可以覆盖原文件。 -h: 人类可读数字排序，能正确比较 2K, 1M, 100G 的大小。 # 查看当前最消耗内存的10个进程 ps aux | sort -rnk 4 | head -10 # 按用户 ID（第三列）从小到大排序 /etc/passwd 文件。 sort -t ':' -nk 3 /etc/passwd 万能管道公式：\n获取数据 (awk/grep/sed) -\u003e 排序 (sort) -\u003e 去重统计 (uniq -c) -\u003e 再次排序 (sort -nr) -\u003e 取顶部结果 (head)","uniq#uniq":"uniq 是一个用于报告或忽略文件中的重复行的过滤器。它的一个关键前提是：输入必须是排序过的，因为 uniq 只会检测相邻的重复行。\n-c --count 在每行前加上该行重复出现的次数。最常用参数，用于统计重复项的频率。 -d --repeated 仅显示重复出现的行（每组重复行只显示一次）。快速找出有哪些内容是重复的。 -u --unique 仅显示不曾重复出现的行（独一无二的行）。找出只出现一次的条目，例如排查异常的单次访问。","元字符#元字符":"常用的元字符：\n. 匹配除了换行符外的任意一个字符 * 匹配任意个跟它前面的字符 [] 匹配方括号中的字符类中的任意一个，比如 [Hh]ello 就可以匹配 hello 和 Hello。 ^ 匹配开头 $ 匹配结尾 \\ 转义字符 扩展元字符：\n+ 匹配前面的正则表达式至少出现一次 ? 匹配前面的正则表达式出现一次或者零次 | 匹配前面或者后面的正则表达式","删除#删除":"sed /匹配模式/d 删除匹配到的行：\nsed '/^#/d' file.txt，删除所有以 # 开头的行 sed 's/^/# /' file.txt，在每一行的行首添加 #，相当于注释掉所有行","常用参数#常用参数":"-n：显示行号 -C：num --context：显示匹配行前后（Context） 各 num 行内容。最常用。 -A num --after-context：显示匹配行之后（After） 的 num 行内容。 -B num\t--before-context：显示匹配行之前（Before） 的 num 行内容。 -r --recursive：递归搜索目录下的所有文件。 -o：只输出匹配到的部分（字符串），而不是输出包含匹配模式的整行。 -E：扩展正则表达式 让 ()、{}、|、+ 等元字符不再需要反斜杠转义。 -i：忽略大小写。 -v：反向选择，即过滤掉匹配指定模式的行，只显示那些不匹配的行。","应用场景#应用场景":"# 在日志中查找所有异常，并显示行号和前后 5 行上下文 grep -n -C 5 -i \"exception\\|error\\|fail\" /var/log/app/app.log # 查找今天的错误 (结合日期过滤) grep \"$(date '+%Y-%m-%d')\" /var/log/app.log | grep -i error # 查找指定时间范围的日志 # -A 999999: 显示匹配行之后的 999999 行 # -B 999999: 显示匹配行之前的 999999 行 grep \"2024-05-24 10:15:\" app.log -A 999999 | grep \"2024-05-24 10:20:\" -B 999999 # 使用 sed（更可靠） sed -n '/2024-05-24 10:15:/,/2024-05-24 10:20:/p' app.log # 使用 awk # 假如日志格式是 YYYY-MM-DD HH:MM:SS [日志内容] # $1 是日期部分（如 2024-05-24），$2 是时间部分（如 10:15:23） # $1\" \"$2 的意思是字符串连接，重新组成完整的时间字符串，然后进行字符串比较。 awk '$1\" \"$2 \u003e= \"2024-05-24 10:15:00\" \u0026\u0026 $1\" \"$2 \u003c= \"2024-05-24 10:20:00\"' app.log # 统计404状态的请求 grep \" 404 \" /var/log/nginx/access.log | wc -l # 找出访问量最大的IP (结合awk和sort) # grep -oE \"\\b([0-9]{1,3}\\.){3}[0-9]{1,3}\\b\" access.log | sort | uniq -c | sort -nr | head -10 # 通过一个唯一的TraceID来追踪一个请求在整个集群中的流转 grep \"abc123-trace-id\" /var/log/microservice/*.log+","应用场景-1#应用场景":"# 有一个用户列表 users.txt，想知道有哪些用户是重复的 sort users.txt | uniq -d # 找出只出现一次的异常条目 grep \"Failed password\" /var/log/secure | sort | uniq -u 日志格式为 [日期 时间] 错误信息。需要统计有哪些类型的错误信息，而不关心它们发生的具体时间。\n[2023-10-27 10:01:23] Connection timeout [2023-10-27 10:05:45] Permission denied [2023-10-27 10:07:12] Connection timeout [2023-10-27 10:08:33] Disk full # 先使用sed/awk处理掉`[`和`]` sed 's/\\[.*\\] //' error.log | sort | uniq -c","应用场景-2#应用场景":"# 将旧 IP 地址替换为新 IP 地址，并创建备份文件 sed -i.bak 's/192.168.1.100/192.168.1.200/g' /etc/nginx/conf.d/*.conf # 修改文件中的路径（注意：分隔符可以用其他字符，如 #，以避免和路径中的 / 冲突） # 旧路径 /old/path 全局替换为新路径 /new/path # `#` 作为分隔符，而不是常见的 `/` sed -i 's#/old/path#/new/path#g' config.ini # 删除配置文件中的所有空行和注释行（以 # 开头） sed -i '/^#/d; /^$/d' /etc/foo.conf # 或者写成多条 -e sed -i -e '/^#/d' -e '/^$/d' /etc/foo.conf # 只查看日志文件中特定时间范围内的行（例如 14:00 到 14:30） sed -n '/May 10 14:00:00/,/May 10 14:30:00/p' /var/log/syslog # 提取 Jenkins 构建日志中 Git 提交的哈希值（假设格式为 Commit: abc123def） cat build.log | sed -n 's/.*Commit: \\([a-f0-9]\\{7,40\\}\\).*/\\1/p' # 提取本机的 IP 地址（假设是 eth0 网卡） ip addr show eth0 | sed -n 's/.*inet \\(192\\.168[^/]*\\).*/\\1/p' # 在脚本中动态修改配置文件中的端口号 NEW_PORT=8080 sed -i \"s/^Port.*/Port $NEW_PORT/\" /etc/ssh/sshd_config # 直接删除文件 script.sh 中所有行尾的回车符（\\r） # \\r 代表回车符（Carriage Return），是 Windows 换行符的一部分 # sed 会读取数据，直到遇到换行符 \\n 为止，不是绝对的文件末尾 sed -i 's/\\r$//' script.sh # 这是一个非常常见的用法！ # 在每行的行首或行尾添加内容 sed -i 's/^/HEADER: /' logfile.txt # 行首添加 sed -i 's/$/\\\\n/' file.txt # 行尾添加换行符（实际是追加\\n字符） 使用扩展正则 -r：为了让命令更清晰易读，建议总是使用 sed -r，避免过多的反斜杠 \\。","应用场景-3#应用场景":"# 提取 ps 命令输出的进程 ID 和命令 (PID 和 CMD) ps aux | awk '{print $2, $11}' | head # 获取所有登录的用户名 who | awk '{print $1}' # 分析 /etc/passwd，提取用户名和使用的 shell awk -F: '{print $1, $7}' /etc/passwd # 分析 /etc/passwd，提取用户名和使用的 shell awk -F: '{print $1, $7}' /etc/passwd # 正则过滤 awk -F \"'\" '/^menu/{ print $1 }' /boot/grub2/grub.cfg menuentry menuentry menuentry # 条件过滤 # 显示磁盘使用率超过 80% 的分区 df -h | awk '$5+0 \u003e 80 {print $1, $5}' # 找出内存使用超过 100MB 的进程 ps aux | awk '$6 \u003e 100000 {print $11, $6/1024 \"MB\"}' # 打印第 5 到第 10 行 awk 'NR\u003e=5 \u0026\u0026 NR\u003c=10' /var/log/syslog # 计算文件总大小（第5列是大小） ls -l | awk '{sum += $5} END {print \"Total Size: \", sum, \"bytes\"}' # 统计日志中每种 HTTP 状态码的出现次数（假设第9列是状态码） awk '{status_count[$9]++} END {for(s in status_count) print s, status_count[s]}' access.log # 计算系统平均负载（最后15分钟是$3） uptime | awk '{print \"15-min load average: \", $NF}' # NF 是最后一个字段 # 格式化输出 /etc/passwd awk -F: 'BEGIN {printf \"%-15s %-10s\\n\", \"Username\", \"Shell\"} {printf \"%-15s %-10s\\n\", $1, $7}' /etc/passwd | head # 为输出添加表头 netstat -tnlp | awk 'BEGIN {print \"Proto Recv-Q Send-Q Local Address\"} NR\u003e2 {print $1, $2, $3, $4}' # 假设 Nginx 访问日志格式为 # 192.168.1.1 - - [10/May/2023:14:12:33 +0800] \"GET /index.html HTTP/1.1\" 200 1234 # 统计每个 IP 的访问次数 awk '{ip_count[$1]++} END {for(ip in ip_count) print ip, ip_count[ip]}' access.log | sort -nr -k2 # 统计最受欢迎的 URL（第7列） awk '{url_count[$7]++} END {for(url in url_count) print url_count[url], url}' access.log | sort -nr | head","打印#打印":"p 打印命令，打印匹配的行。通常与 -n 选项一起使用。\nsed -n '/error/p' /var/log/syslog，只打印包含 error 的行，相当于 grep \"error\"。","插入文本#插入文本":"i\\text：在指定行前插入文本。\nsed '3i\\# This is a new line' file.txt，在第 3 行之前插入一行注释 。","替换#替换":"sed 's/old/new/' filename，将文件中所有的 old 替换为 new。\nsed -e 's/old/new/' -e 's/old/new/' filename ...，可以执行多次替换脚本但是不能省略 -e。 sed -i 's/old/nnew/' 's/old/new/' filename ...，直接修改文件内容 sed 's/foo/bar/g' file.txt 将全文所有的 foo 替换为 bar g：全局替换，不加 g 只替换每一行中第一个匹配到的 “foo” p：打印发生替换的那一行。与 -n 选项连用。 i 或 I：忽略大小写进行匹配。","替换整行#替换整行":"c\\text：替换整行文本。\nsed '/old_line/c\\This is the new line content' file.txt","行范围#行范围":"n：第 n 行。sed '5s/foo/bar/' 只替换第 5 行的 foo n,m：第 n 到 m 行。sed '10,20s/foo/bar/g' $：最后一行。sed '$s/foo/bar/' /regexp/：匹配正则表达式的行。sed '/start/,/end/d' 删除从包含 start 的行到包含 end 的行之间的所有内容。","表达式#表达式":"赋值操作符：\n=，var1 = \"name\"，var2 = $1 ++ -- += -= *= /+ %= ^= 算数操作符：\n+ - * / % ^ 系统变量：\nFS 字段分隔符，默认是空格和制表符。 RS 行分隔符，用于分割每一行，默认是换行符。 OFS 输出的字段分隔符，用于打印时分隔字段，默认为空格。 ORS 输出行分隔符，用于打印时分隔记录，默认为换行符。 NR 表示当前处理的是第几行。 FNR 行数。 NF 字段数量，所以最后一个字段内容可以用 $NF 取出，$(NF-1) 代表倒数第二个字段。 [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{FS=\":\"}{print $1}' # BEGIN{FS=\":\"} 表示在读入之前设置字段分隔符为 :，也可以写成 awk -F \":\" '{print $1}' root bin daemon adm lp [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{FS=\":\"}{print $1,$2}' root x # 可以看出输出的字段分隔符默认为空格 bin x daemon x adm x lp x [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{FS=\":\";OFS=\"-\"}{print $1,$2}' # 输出的字段分隔符设置为 - root-x bin-x daemon-x adm-x lp-x [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{RS=\":\"}{print $0}' # 已 : 为行分隔符，输出每一行 root x 0 0 root /root /bin/bash bin x 1 1 bin /bin /sbin/nologin daemon [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk '{print NR}' # 显示行号 1 2 3 4 5 [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk '{print NR, $0}' 1 root:x:0:0:root:/root:/bin/bash 2 bin:x:1:1:bin:/bin:/sbin/nologin 3 daemon:x:2:2:daemon:/sbin:/sbin/nologin 4 adm:x:3:4:adm:/var/adm:/sbin/nologin 5 lp:x:4:7:lp:/var/spool/lpd:/sbin/nologin [root@SGDLITVM0905 ~]# awk '{print FNR, $0}' /etc/hosts /etc/hosts # FNR 会重排行号 1 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 2 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3 4 16.187.191.150 SGDLITVM0905.hpeswlab.net SGDLITVM0905 1 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 2 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3 4 16.187.191.150 SGDLITVM0905.hpeswlab.net SGDLITVM0905 You have new mail in /var/spool/mail/root [root@SGDLITVM0905 ~]# awk '{print NR, $0}' /etc/hosts /etc/hosts # FNR 不会重排行号 1 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 2 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3 4 16.187.191.150 SGDLITVM0905.hpeswlab.net SGDLITVM0905 5 127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 6 #::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 7 8 16.187.191.150 SGDLITVM0905.hpeswlab.net SGDLITVM0905 [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{FS=\":\"}{print NF}' # NF 输出字段数量 7 7 7 7 7 You have new mail in /var/spool/mail/root [root@SGDLITVM0905 ~]# head -5 /etc/passwd | awk 'BEGIN{FS=\":\"}{print $NF}' # $NF 就可以获取到最后一个字段的内容 /bin/bash /sbin/nologin /sbin/nologin /sbin/nologin /sbin/nologin 关系操作符：\n\u003c \u003e \u003c= \u003e= == != ~ !~ 布尔操作符：\n\u0026\u0026 || ! 条件语句：\nif (表达式) awk 语句1 [ else awk 语句2 ] 多个语句可以使用 {} 括起来。\n[root@SGDLITVM0905 ~]# cat score.txt user1 60 61 62 63 64 65 user2 70 71 72 73 74 75 user3 80 81 82 83 84 85 user4 90 91 92 93 94 95 [root@SGDLITVM0905 ~]# awk '{if($2\u003e=80) print $1}' score.txt user3 user4 [root@SGDLITVM0905 ~]# awk '{if($2\u003e=80) print $1; print $2}' score.txt # 这种写法会把所有的第二个字段输出 60 70 user3 80 user4 90 [root@SGDLITVM0905 ~]# awk '{if($2\u003e=80) {print $1; print $2} }' score.txt # 如果想一起输出要加上 {} ，多个语句一起执行 60 70 user3 80 user4 90 while 循环：\nwhile(表达式) awk 语句1 do 循环：\ndo { awk 语句1 }while(表达式) for 循环：\nfor(初始值;判断条件;累加) awk 语句1 可以使用 break 和 continue。\n[root@SGDLITVM0905 ~]# cat score.txt user1 60 61 62 63 64 65 user2 70 71 72 73 74 75 user3 80 81 82 83 84 85 user4 90 91 92 93 94 95 [root@SGDLITVM0905 ~]# head -1 score.txt user1 60 61 62 63 64 65 [root@SGDLITVM0905 ~]# head -1 score.txt | awk 'for(c=2;c\u003c=NF;c++) print c' 2 3 4 5 6 7 [root@SGDLITVM0905 ~]# head -1 score.txt | awk 'for(c=2;c\u003c=NF;c++) print $c' # 输出值 61 62 63 64 65 [root@SGDLITVM0905 ~]# head -1 score.txt | awk 'for(c=2;c\u003c=NF;c++) print $c' # 输出值 61 62 63 64 65 数组：\n数组[下标] = 值，初始化数组。下标可以是数字，也可以是字符串。 for (变量 in 数组)，数组[变量] 获取数组元素 delete 数组[下标] 删除数组元素 [root@SGDLITVM0905 ~]# cat score.txt user1 60 61 62 63 64 65 user2 70 71 72 73 74 75 user3 80 81 82 83 84 85 user4 90 91 92 93 94 95 [root@SGDLITVM0905 ~]# awk '{ sum=0; for(column=2;column\u003c=NF;column++) sum+=$column; print sum }' score.txt # 计算每个人的总分 375 435 495 555 [root@SGDLITVM0905 ~]# [root@SGDLITVM0905 ~]# awk '{ sum=0; for(column=2;column\u003c=NF;column++) sum+=$column; avg[$1]=sum/(NF-1); }END{ for( user in avg) print user, avg[user]}' score.txt # 计算每个人的平均分 并在 END 例程中格式化输出 user1 62.5 user2 72.5 user3 82.5 user4 92.5 awk 脚本可以保存到文件：\n[root@SGDLITVM0905 ~]# awk -f avg.awk score.txt user1 62.5 user2 72.5 user3 82.5 user4 92.5 -f 加载 awk 文件 avg.awk 文件的内容：{ sum=0; for(column=2;column\u003c=NF;column++) sum+=$column; avg[$1]=sum/(NF-1); }END{ for( user in avg) print user, avg[user]}。 命令行参数数组：\nARGC 命令行参数数组的长度 ARGV 命令行参数数组 [root@SGDLITVM0905 ~]# cat arg.awk BEGIN{ for(x=0;x","追加#追加":"a\\text：在指定行后追加文本。\nsed '/server_name www.example.com;/a\\ return 301 https://$host$request_uri;' nginx.conf，在 server_name 行后追加一条 301 重定向规则。"},"title":"操作文本"},"/devops-learn/docs/linux/04_find/":{"data":{"":"","find#find":"find 命令用来在指定目录下查找文件。find \u003c文件路径\u003e \u003c查找条件\u003e [补充条件]。\n[root@pooky ~]# find /etc -name pass* # 查找 /etc 目录下 pass 前缀的文件 /etc/pam.d/passwd /etc/pam.d/password-auth-ac /etc/pam.d/password-auth /etc/openldap/certs/password /etc/passwd /etc/selinux/targeted/active/modules/100/passenger /etc/passwd- [root@pooky ~]# find /etc -regex .*wd$ # 使用正则 -regex /etc/security/opasswd /etc/pam.d/passwd /etc/passwd [root@pooky ~]# find /etc -type f -regex .*wd$ /etc/security/opasswd /etc/pam.d/passwd /etc/passwd 常用参数：\n-name：按文件名查找（区分大小写）。 -iname：按文件名查找（不区分大小写）。 -type：按文件类型查找。find /var -type d -name \"log\" 在 /var 下查找所有名为 log 的目录。 f：普通文件 (file) d：目录 (directory) l：符号链接 (link) -mtime：按文件内容修改时间查找（天为单位），find /var/log -name \"*.log\" -mtime -1，查找 /var/log 下昨天到现在修改过的日志文件。 -mtime +n：n 天之前被修改过的文件 -mtime -n：n 天之内被修改过的文件 -mtime n：正好 n 天前被修改过的文件 -mmin：按文件内容修改时间查找（分钟为单位），find /tmp -cmin -10 查找 /tmp 下 10 分钟之内内容被修改过的文件 -atime/-amin：按文件访问时间查找（天/分钟） -ctime/-cmin：按文件状态改变时间查找（如权限、属主） -size：按文件大小查找，find / -type f -size +100M 在整个系统查找大于 100MB 的文件（常用于定位磁盘空间杀手） -size +n：大于 n 个指定单位的文件 find /var/log -name \"*.log\" -size +1G 查找超过 1G 的巨大日志文件 -size -n：小于 n 个指定单位的文件 单位：c（字节），k（KB），M（MB），G（GB） -perm：按文件权限查找 -perm 644：查找权限正好是 644 的文件 -perm -644：查找权限包含 644 的文件（如 755, 644 都匹配，因为都包含了 rw-r–r–） -user：按文件属主查找，find /home -user pooky 查找属于用户 pooky 的所有文件 -group：按文件属组查找","whereis#whereis":"在一个标准的 Linux 系统目录列表（如 /bin, /usr/bin, /usr/local/bin, /usr/share/man 等）中搜索某个命令的二进制文件（可执行文件）、源代码文件和 man 帮助手册文件。\n-b 只搜索二进制（可执行）文件。\twhereis -b nginx。 -m 只搜索手册页文件。whereis -m ls。 -s 只搜索源代码文件。whereis -s bash。","which#which":"在用户的 PATH 环境变量所指定的目录列表中，搜索某个可执行命令的完整路径。\nwhich 只找能直接运行的命令。\n-a 显示所有匹配的可执行文件路径，而不仅仅是第一个。 用 which 检查一下，如果没输出，说明该命令确实不在 PATH 中。","应用场景#应用场景":"# 清理 /tmp 下超过 7 天未访问的临时文件 find /tmp -type f -atime +7 -delete # 清理 /var/log 下超过 30 天的日志文件（.log.gz 等压缩过的日志） find /var/log -name \"*.log*\" -mtime +30 -delete # 查找当前目录下大于 500MB 的文件，并按大小排序 find . -type f -size +500M -exec ls -lh {} \\; | sort -k 5 -hr # 查找所有 .conf 配置文件，并用 tar 打包备份 find /etc -name \"*.conf\" -exec tar -rvf backup.tar {} \\; # 将某个用户（如nginx）创建的所有文件属主改为 www-data find /srv/www -user nginx -exec chown www-data:www-data {} \\; # 批量修改目录权限为 755，文件权限为 644 find /path/to/dir -type d -exec chmod 755 {} \\; find /path/to/dir -type f -exec chmod 644 {} \\; # 统计当前目录下 JavaScript 文件的数量 find . -name \"*.js\" | wc -l # 忽略错误输出：在搜索根目录 / 时，会遇到大量权限拒绝的错误，干扰查看结果。可以将错误重定向到黑洞。 find / -name \"something\" 2\u003e/dev/null 先确认，再操作：在使用 -exec、-delete 等具有破坏性的参数前，先用 -print 或 -ls 模拟运行一次，确认找到的文件是你要操作的目标。","执行操作#执行操作":"-exec：对匹配的文件执行指定的命令。命令以 ; 结束，{} 是查找结果的占位符。; 需要转义。 -ok：与 -exec 类似，但在执行命令前会交互式地询问用户确认，更安全。 -delete：直接删除匹配的文件。 # 查找并删除 /tmp 下所有 .tmp 文件 find /tmp -name \"*.tmp\" -exec rm -f {} \\;","逻辑操作符#逻辑操作符":"-a 或 -and：与（默认操作符，可省略） -o 或 -or：或 ! 或 -not：非 ()：组合条件，提高优先级。括号需要被转义或引用，如 \\( ... \\) 或 '( ... )' # 查找所有 .txt 或 .md 文件 find . \\( -name \"*.txt\" -o -name \"*.md\" \\) # 查找所有不属于 root 用户的文件 find . ! -user root"},"title":"查找文件"},"/devops-learn/docs/linux/05_cpu/":{"data":{"":"","ps#ps":"查看进程状态快照。\nps aux：(最常用) 显示所有用户的所有进程的详细信息。相比较 ps -ef 多了两列 %CPU 和 %MEM，少了一列 PPID。 ps -ef：显示所有进程的完整格式信息。 ps -eF：显示所有进程的更详细的完整格式信息。 ps -eo：自定义输出列。 ps -eLf：显示线程。 # 查找 nginx 或 java 进程 ps aux | grep nginx ps aux | grep java # 按 CPU 使用率降序排列 # sort 参数可能有些系统不支持，sort 命令功能更丰富 ps aux --sort=-%cpu | head -10 # 按内存使用率降序排列 ps aux --sort=-%mem | head -10 # 查看进程的父进程 ID （PPID） ps -eo pid,ppid,cmd | grep \u003c进程名\u003e # 检查僵尸进程 ps aux | awk '$8==\"Z\" {print $0}'","pstree#pstree":"以树状结构显示进程之间的父子关系。\n-p，--pid (最常用) 显示进程ID（PID）。 -a，--arguments 显示命令行参数。 -u，--user 显示用户信息。 -H，--highlight 高亮显示指定进程及其祖先。 使用场景：\n直观查看系统的进程层次结构 排查“杀不掉的进程” 如果一个进程反复被拉起，用 pstree 可以清晰地看到它的父进程是谁，从而找到根源并处理父进程。 分析由守护进程（如 systemd, supervisord）管理的服务查看服务的所有子进程是否正常启动。","top#top":"top 显示进程和系统信息。其实和 ps 差不多，只不过显示的是实时数据。它是交互式的。\n常用交互指令:\nP (大写) 按 CPU 使用率排序 M (大写) 按 内存使用率（RES） 排序 N (大写) 按 PID 排序 T (大写) 按 CPU 时间（TIME+） 排序 k 终止一个进程（会提示输入PID） r renice（修改进程优先级） 1 展开显示所有CPU核心的详细使用情况 top 命令进程的几种状态：\nR 运行（正在运行或等待运行） S 中断（睡眠） D 不可中断（等待 IO） Z 僵尸（已终止，但父进程未回收） T 停止（已停止） N 不可见（没有显示）","三者的协同工作流#三者的协同工作流":"top：发现问题。发现系统负载很高，CPU %wa（等待 IO）指标飙升。 在 top 中按 M：初步定位。发现没有进程占用特别高的内存或CPU。 ps aux：深入分析。结合 grep 和状态过滤，寻找处于 D（不可中断睡眠）状态的进程，这通常是导致高 IO 等待的根源 ps aux | awk '$8==\"D\" {print $0}'， 查找 D 状态的进程 pstree -p：追溯根源。如果找到问题进程，用 pstree 查看是谁启动了它，判断影响范围，决定是终止子进程还是父进程。 kill -9 ：强制终止进程。如果终止后问题没有解决，可能需要重启系统。 top：再次检查。确认问题是否解决。","进程管理#进程管理":"ps：拍一张静态照片，用于详细分析、查找和记录。 top：打开实时直播，用于动态监控和即时操作。 pstree：画一张家谱/关系图，用于理解进程间的依赖和层次关系。"},"title":"CPU"},"/devops-learn/docs/linux/08_net/":{"data":{"":"net-tools 是 CentOS 7 之前的版本使用的网络管理工具，而 iproute2 是 CentOS 7 之后主推的网络管理工具。\nnet-tools 包括：\nifconfig 网卡配置 route 网关配置 netstat 查看网络状态 iproute2 包括：\nip：包含里 ifconfig 和 route 的功能 ss：类似 netstat ，更快更强","ip-addr-命令#ip addr 命令":"ip addr 命令用于查看和管理网络接口的 IP 地址。","ip-route-命令#ip route 命令":"ip route 命令它不仅仅是用来“查看路由表”，更是操作 Linux 内核路由表的核心工具。","ip-route-命令的角色#ip route 命令的角色":"ip route 是 iproute2 软件套件的一部分，用于查看和管理内核中的路由表（Routing Table）。它的工作原理可以理解为用户空间与内核空间之间的一个桥梁：\n用户输入命令：例如 ip route add 192.168.2.0/24 via 192.168.1.1。 内核系统调用：通过 Netlink 套接字（一种专门用于内核与用户进程通信的机制）将“添加路由”的请求发送给 Linux 内核。 内核处理请求：内核网络栈接收到请求后，会根据请求参数（如目标网络、下一跳地址等）进行解析和处理。 内核修改路由表：内核网络栈接收到请求后，在其内部的路由表中创建、删除或修改相应的路由条目。 生效：此后，所有进出主机的数据包转发决策都将依据新的路由表进行。","ip-命令#ip 命令":"","mtr#mtr":"运行 mtr 可以查看更详细的网络状态：\n类别 参数 作用 示例 协议/端口 -T TCP SYN 探测 mtr -T -P 443 1.1.1.1 -u UDP 探测 mtr -u -P 53 8.8.8.8 -P 指定目标端口 mtr -T -P 443 1.1.1.1 输出格式 -n 不解析主机名（纯 IP） mtr -n 8.8.8.8 -r 报告模式（一次性统计） mtr -r 8.8.8.8 -c 发送 N 次探测后结束 mtr -r -c 100 8.8.8.8 时间控制 -i 每次探测间隔（秒） mtr -i 0.5 8.8.8.8 -p 同 -i（兼容写法） mtr -p 0.5 8.8.8.8 TTL/跳数 -m 最大跳数（默认 30） mtr -m 50 8.8.8.8 调试/帮助 -4 / -6 强制 IPv4 / IPv6 mtr -4 8.8.8.8 -h 查看全部选项 mtr -h mtr -n baidu.com 输出示例：\nMy traceroute [v0.95] DESKTOP-DMAGDPE (172.18.149.141) -\u003e bdidu.com (76.223.54.146) 2025-08-27T15:34:02+0800 Keys: Help Display mode Restart statistics Order of fields quit Packets Pings Host Loss% Snt Last Avg Best Wrst StDev 1. 172.18.144.1 0.0% 9 0.6 0.7 0.2 1.5 0.4 2. 192.168.31.1 0.0% 9 2.1 2.5 1.5 4.2 1.0 3. 192.168.1.1 0.0% 9 4.0 3.3 2.1 4.0 0.6 4. 100.84.128.1 0.0% 9 6.4 8.7 5.4 19.5 4.2 5. (waiting for reply) 6. (waiting for reply) 7. (waiting for reply) 8. (waiting for reply) 9. 219.158.3.214 50.0% 9 30.4 31.1 30.2 32.7 1.2 10. 219.158.3.102 33.3% 9 62.7 61.7 60.3 62.7 0.9 11. 219.158.34.230 0.0% 9 86.4 88.9 85.4 107.6 7.6 12. (waiting for reply) 13. (waiting for reply) 14. (waiting for reply) 15. 52.93.8.41 0.0% 8 95.2 94.8 92.7 99.4 2.2 16. 52.93.8.18 0.0% 8 86.7 87.8 85.8 94.2 2.9 17. 76.223.54.146 0.0% 8 85.9 86.3 84.9 88.5 1.1 mtr 输出列解读：\nLoss%：丢包率。这是最重要的指标，某一跳的丢包率突然升高，通常意味着该节点或链路有问题。 Host：主机名或 IP 地址。 Snt：发送的探测包数量。 Last：最近一次的延迟（毫秒）。 Avg：平均延迟（毫秒）。 Best：最佳延迟（毫秒）。 Wrst：最差延迟（毫秒）。 StDev：延迟抖动（标准方差）。这个值越大，说明网络越不稳定。","ss-命令#ss 命令":"属于 iproute2 工具包，类似 netstat，参数类似，显示格式不同。\n两者的区别：\n性能：ss 直接从内核空间获取信息，速度极快。而 netstat 会遍历 /proc/net 下的文件，在连接数非常多时（如数万）速度很慢。 信息更丰富：ss 可以显示更多的 TCP 内部状态信息（如拥塞控制、内存使用等）。 格式为：ss [选项] [过滤器]\n-t：显示 TCP sockets。ss -t。 -u：显示 UDP sockets。ss -u。 -l：显示正在监听（Listen）的 sockets。ss -tl。 -a：显示所有 sockets（包括监听和非监听）。ss -ta。 -n：不解析服务名称（显示端口号而不是像“http”这样的名字）。强烈推荐始终使用，速度更快且信息更准确。ss -tn。 -p：显示使用 socket 的进程信息（需要 sudo）。sudo ss -tp。 -4：仅显示 IPv4 sockets。ss -t4。 -6：仅显示 IPv6 sockets。ss -t6。 过滤选项:\nsport = :端口号：过滤源端口。ss sport = :80 dport = :端口号：过滤目标端口。ss dport = :443 src IP地址：过滤源 IP 地址。ss src 192.168.1.100 dst IP地址：过滤目标 IP 地址。ss dst 8.8.8.8 state：过滤连接状态。ss state established 输出选项:\n-e：显示详细的 socket 信息（如 UID、内存等）。ss -te。 -o：显示 TCP 计时器信息（如 TCP 重传超时）。ss -to。 -i：显示 TCP 内部信息（拥塞控制、流量控制）。ss -ti。 -m：显示 socket 的内存使用情况。ss -tm。 -s：打印摘要统计信息（非常有用）。ss -s。","tcpdump#tcpdump":"tcpdump -i any 07:02:12.195611 IP test.ya.local.59915 \u003e c2.shared.ssh: Flags [.], ack 1520940, win 2037, options [nop,nop,TS val 1193378555 ecr 428247729], length 0 07:02:12.195629 IP c2.shared.ssh \u003e test.ya.local.59915: Flags [P.], seq 1520940:1521152, ack 1009, win 315, options [nop,nop,TS val 428247729 ecr 1193378555], length 212 07:02:12.195677 IP test.ya.local.59915 \u003e c2.shared.ssh: Flags [.], ack 1521152, win 2044, options [nop,nop,TS val 1193378555 ecr 428247729], length 0 07:02:12.195730 IP c2.shared.ssh \u003e test.ya.local.59915: Flags [P.], seq 1521152:1521508, ack 1009, win 315, options [nop,nop,TS val 428247730 ecr 1193378555], length 356 -i：指定网卡，any 表示任意网卡。如果只需要查看某个网卡的数据包，例如 ehh0，使用 tcpdump -i eth0。","traceroute#traceroute":"traceroute -w 1 www.baidu.com，\n-w 等待响应的超时时间，单位为秒，-w 1 表示某个 IP 超时的最大等待时间为 1 秒。 -n 显示 IP 地址 -m 设置最大跳数，默认 64。 -q 每个网关发送数据包个数，默认 3。 -p 指定使用的目标端口。 -I，--icmp 使用 ICMP ECHO 作为探测包。 -M，--type=Method 指定使用的探测方法（icmp 或者 udp），默认 udp。 [root@shcCDFrh75vm8 ~]# traceroute -w 1 www.baidu.com traceroute to www.baidu.com (104.193.88.77), 30 hops max, 60 byte packets 1 gateway (16.155.192.1) 0.525 ms 0.635 ms 0.800 ms 2 10.132.24.193 (10.132.24.193) 0.600 ms 0.930 ms 1.137 ms 3 192.168.201.122 (192.168.201.122) 0.826 ms 0.746 ms 0.674 ms 4 192.168.200.45 (192.168.200.45) 2.045 ms 1.978 ms 1.962 ms 5 192.168.203.249 (192.168.203.249) 29.375 ms 29.351 ms 29.275 ms 6 192.168.203.250 (192.168.203.250) 3.892 ms 2.980 ms 2.907 ms 7 192.168.200.185 (192.168.200.185) 68.675 ms 68.636 ms 68.691 ms 8 192.168.200.186 (192.168.200.186) 70.094 ms 70.356 ms 69.995 ms 9 * * * 10 * * * # * 表示不支持 traceroute 追踪。 11 * * * 12 * * * 13 * * * 14 * * * 15 * * * 16 * * * 17 * * * 18 * * * 19 * * * 记录按序列号从 1 开始，每个纪录就是一跳 ，每跳表示一个网关，可以看到每行有三个时间，单位是 ms，之所以是 3 个，其实就是 -q 的默认参数。\n探测数据包向每个网关发送三个数据包后，记录网关响应后返回的时间。","什么是路由#什么是路由？":"它是一套双向规则，既扮演着“迎宾员”的角色，决定如何接收外来数据包；也扮演着“导航员”的角色，决定如何发送外出数据包。","使用场景#使用场景":"用于检查哪些端口已开放并正在等待连接：\nsudo ss -tunlp # -t: TCP, -u: UDP, -n: 不解析, -l: 监听, -p: 显示进程 # 输出 # 一眼就能看出：Nginx（PID 1234）在监听 80 端口，SSH（PID 5678）在监听 22 端口 Netid State Recv-Q Send-Q Local Address:Port Peer Address:Port tcp LISTEN 0 128 *:80 *:* users:((\"nginx\",pid=1234,fd=6)) tcp LISTEN 0 128 *:22 *:* users:((\"sshd\",pid=5678,fd=3)) # 查看所有活跃的网络连接 ss -tna | grep ESTAB # 或者更精确地，查看所有已建立连接 ss -t state established 当服务器出现性能问题或怀疑连接数过多时：\nss -s # 输出列解读： Total: 1234 (kernel 0) # TCP 行：可以快速了解当前连接状态分布（已建立、关闭、等待释放等） TCP: 1456 (estab 234, closed 1100, orphaned 5, timewait 1100) Transport Total IP IPv6 * 0 - - RAW 1 0 1 UDP 20 18 2 TCP 356 350 6 INET 377 368 9 FRAG 0 0 0 排查特定服务或端口的问题：\n# 1. 查看谁在连接我的MySQL服务（3306端口） ss -tn dst :3306 # 2. 查看我的Web服务器（80端口）正在与哪些客户端建立连接 ss -tn src :80 # 3. 查看某个IP地址（如 192.168.1.15）与我的所有连接 ss -tn dst 192.168.1.15 # 分析TCP连接内部状态 # 显示TCP连接的详细状态，包括计时器信息（重传超时等） ss -tno # 显示更详细的内部信息，包括拥塞窗口、往返时间等 ss -tni","使用场景-1#使用场景":"# 精确定位网络丢包和抖动问题（最核心的场景） # 运行后，观察哪一跳的 Loss% 开始出现并持续存在（而不是仅在最后一跳），这里就是问题点。 mtr -n baidu.com # 向目标发送100个包后生成报告 # 网络不稳定时，运行一段时间（几分钟）的 mtr，将报告保存下来，作为向运营商报障的有力证据。 mtr -r -c 100 -n example.com \u003e mtr_report.log # 只显示最重要的列：丢包率、平均延迟、最佳延迟、最差延迟 mtr -r -c 50 -o \"L ABW\" -n 203.0.113.10 有些网络设备会限速或优先处理数据包而非 ICMP 应答，导致中间节点显示丢包，但最终目标不丢包。这就需要经验来判断是真实丢包还是设备策略。","分组#分组":"如果要抓取：来源 ip 为 10.211.55.10 且目标端口为 3306 或 6379 的包，如果按照下面的方式写，就会报错：\ntcpdump -i any src 10.211.55.10 and (dst port 3306 or 6379) 因为 () 是不允许的。这时候可以使用单引号 ' 把复杂的组合条件包起来：\ntcpdump -i any 'src 10.211.55.10 and (dst port 3306 or 6379)'","只抓取-5-个报文#只抓取 5 个报文":"使用 -c number 命令可以抓取 number 个报文后退出。\ntcpdump -i any -nn port 80 -c 5","如何查看到目标主机的网络状态#如何查看到目标主机的网络状态":"使用 ping 查看网络是否是通的。 traceroute 和 mtr 辅助 ping 命令，在 ping 通网络之后，如果网络通信还是有问题， 可以使用 traceroute 可以查看网络中每一跳的网络质量。 mtr 可以检测网络中是否有丢包。 nslookup 查看域名对应的 IP。 如果主机可以连接，但是服务仍然无法访问，使用 telnet 检查端口状态。 如果端口没有问题，仍然无法访问，可以使用 tcpdump 进行抓包，更细致的查看网络问题。 使用 netstat 和 ss，查看服务范围。","常用命令示例及其原理#常用命令示例及其原理":"命令示例 工作原理 ip route add default via 192.168.1.1 添加默认路由。将所有未知流量导向网关 192.168.1.1。 ip route add 10.1.0.0/16 via 192.168.1.2 添加静态路由。将所有去往 10.1.0.0/16 的流量交给下一跳路由器 192.168.1.2。 ip route del 10.1.0.0/16 删除一条静态路由。 ip route replace default via 192.168.1.254 替换现有默认路由。将默认网关从之前的改为 192.168.1.254。 ip route get 8.8.8.8 模拟路由查询，诊断“网络不通”问题。显示内核是如何路由到 8.8.8.8 的，用于调试。","显示所有的-rst-包#显示所有的 RST 包":"tcpdump 'tcp[13] \u0026 4 != 0' TCP 首部中 offset 为 13 的字节的第 3 比特位就是 RST。tcp[13] 表示 tcp 头部中偏移量为 13 字节。!=0 表示当前 bit 置 1，即存在此标记位，跟 4 做与运算是因为 RST 在 TCP 的标记位的位置在第 3 位(00000100)。","显示绝对序号#显示绝对序号":"默认情况下，tcpdump 显示的是从 0 开始的相对序号。如果想查看真正的绝对序号，可以用 -S 选项。\n# 没有 -S tcpdump -i any port 80 -nn 12:12:37.832165 IP 10.211.55.10.46102 \u003e 36.158.217.230.80: Flags [P.], seq 1:151, ack 1, win 229, length 150 12:12:37.832272 IP 36.158.217.230.80 \u003e 10.211.55.10.46102: Flags [.], ack 151, win 16384, length 0 # 加了 -S tcpdump -i any port 80 -nn -S 12:13:21.863918 IP 10.211.55.10.46074 \u003e 36.158.217.223.80: Flags [P.], seq 4277123624:4277123774, ack 3358116659, win 229, length 150 12:13:21.864091 IP 36.158.217.223.80 \u003e 10.211.55.10.46074: Flags [.], ack 4277123774, win 16384, length 0","禁用主机与端口解析#禁用主机与端口解析":"不加 -n 选项，tcpdump 会显示主机名，比如下面的 test.ya.local 和 c2.shared：\n09:04:56.821206 IP test.ya.local.59915 \u003e c2.shared.ssh: Flags [P.], seq 397:433, ack 579276, win 2048, options [nop,nop,TS val 1200089877 ecr 435612355], length 36 加上 -n 选项以后，可以看到主机名都已经被替换成了 ip：\ntcpdump -i any -n 10:02:13.705656 IP 10.211.55.2.59915 \u003e 10.211.55.10.ssh: Flags [P.], seq 829:865, ack 1228756, win 2048, options [nop,nop,TS val 1203228910 ecr 439049239], length 36 常用端口还是会被转换成协议名，比如 ssh 协议的 22 端口。如果不想 tcpdump 做转换，可以加上 -nn，这样就不会解析端口了，输出中的 ssh 变为了 22：\ntcpdump -i any -nn 10:07:37.598725 IP 10.211.55.2.59915 \u003e 10.211.55.10.22: Flags [P.], seq 685:721, ack 1006224, win 2048, options [nop,nop,TS val 1203524536 ecr 439373132], length 36","网络诊断#网络诊断":"","路由查询过程#路由查询过程":"当内核需要发送一个数据包时，是如何使用这张表的呢？这个过程称为路由查找 (Route Lookup)，它遵循最长前缀匹配 (Longest Prefix Match) 原则：\n数据包目标 IP：例如 192.168.0.150。 查询路由表：内核逐条比对路由条目中的“目标网络”。 匹配： 它会同时匹配 192.168.0.0/24 (24 位前缀) 和 default / 0.0.0.0/0 (0 位前缀)。 根据最长前缀匹配原则，/24 比 /0 更具体、更长，因此优先级更高。 执行：内核选择 192.168.0.0/24 dev eth0 这条路由，直接将数据包从 eth0 接口发出，而无需经过网关。","路由表的结构与查看#路由表的结构与查看":"直接输入 ip route show（或简写为 ip r）可以查看当前的路由表。它的结构由一系列路由条目组成，每个条目包含以下几个关键部分：\n部分 含义 示例 解释 目标网络 (Destination) 数据包要去的IP网段 192.168.1.0/24 这条规则适用于去往 192.168.1.x 的所有包 via (网关/Gateway) 下一跳路由器的IP地址 via 192.168.0.1 把数据包发给这个地址（路由器）去处理 dev (接口/Device) 数据包应该从哪个网络接口发出 dev eth0 数据包从 eth0 网卡发出 proto (协议/Protocol) 此路由条目的来源 proto kernel 由内核自动生成（直连路由） scope (作用域) 路由的有效范围 scope link 仅适用于直连链路 metric (度量值) 路由的优先级（成本） metric 100 值越小，优先级越高 ip route show 示例：\ndefault via 192.168.0.1 dev eth0 proto dhcp metric 100 192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.100 metric 100 local 192.168.0.100 dev eth0 proto kernel scope host src 192.168.0.100 默认路由 (Default Route)：default via 192.168.0.1 dev eth0 proto dhcp metric 100。 所有发往非本机、非直连网络的数据包（即不知道往哪扔的包），都通过 eth0 网卡发给网关 192.168.0.1。 default 是 0.0.0.0/0 的简写。 proto dhcp 表示这个路由是通过 DHCP 动态获取的。 metric 100 表示这个路由的优先级是 100，数值越小，优先级越高。 直连路由 (Direct Route)：192.168.0.0/24 dev eth0 proto kernel scope link src 192.168.0.100 metric 100。 发往 192.168.0.0/24 这个网段的数据包，直接通过 eth0 网卡发出，不需要网关。src 指明了从本机发出数据包时，默认使用的源 IP 地址。 当你为网卡配置 IP 地址并启动时（如 ip addr add 192.168.0.100/24 dev eth0），内核会自动生成对应的直连路由。 意思就是，这个包是局域网内的包，不需要通过网关发送了，所以没有 via。 本地路由 (Local Route)：local 192.168.0.100 dev eth0 proto kernel scope host src 192.168.0.100。 这条路由通常不会在 ip route show 中直接显示，需要使用 ip route show table local 查看。它管理发往本机自身 IP 的数据包。 local：关键字。明确标识这是一条本地路由，与普通路由的网段格式（如 192.168.0.0/24）不同。 192.168.0.100：目标地址。这就是本机配置的 IP 地址。它是一个精确的主机路由（/32），而不是一个网段。 scope host：作用域。这是最关键的部分！scope host 意味着这条路由仅在本机内部有效，绝不可能被转发到网络上去。它划清了一条清晰的边界：这是“我自己”。 dev lo：关联接口。这个 IP 地址被配置在 lo 接口上。\nproto kernel：协议。由内核自动生成。\nscope host：作用域。这是最关键的部分！scope host 意味着这条路由仅在本机内部有效，绝不可能被转发到网络上去。它划清了一条清晰的边界：这是“我自己”。","输出-ascii-格式#输出 ASCII 格式":"-A 用 ASCII 打印报文内容：\ntcpdump -i any -nn port 80 -A 11:04:25.793298 IP 183.57.82.231.80 \u003e 10.211.55.10.40842: Flags [P.], seq 1:1461, ack 151, win 16384, length 1460 HTTP/1.1 200 OK Server: Tengine Content-Type: application/javascript Content-Length: 63522 Connection: keep-alive Vary: Accept-Encoding Date: Wed, 13 Mar 2019 11:49:35 GMT Expires: Mon, 02 Mar 2020 11:49:35 GMT Last-Modified: Tue, 05 Mar 2019 23:30:55 GMT ETag: W/\"5c7f06af-f822\" Cache-Control: public, max-age=30672000 Access-Control-Allow-Origin: * Served-In-Seconds: 0.002","输出到文件#输出到文件":"tcpdump -i any port 80 -w test.pcap 生成的 pcap 文件就可以用 wireshark 打开进行更详细的分析。","过滤-syn--ack-包#过滤 SYN + ACK 包":"tcpdump 'tcp[13] \u0026 18 != 0'","过滤主机#过滤主机":"如果只想查看 ip 为 10.211.55.2 的网络包，这个 ip 可以是源地址也可以是目标地址：\ntcpdump -i any host 10.211.55.2","过滤协议#过滤协议":"只查看 udp 协议：\ntcpdump -i any -nn udp 10:25:31.457517 IP 10.211.55.10.51516 \u003e 10.211.55.1.53: 23956+ A? www.baidu.com. (31) 10:25:31.490843 IP 10.211.55.1.53 \u003e 10.211.55.10.51516: 23956 3/13/9 CNAME www.a.shifen.com., A 14.215.177.38, A 14.215.177.39 (506)","过滤指定端口范围内的流量#过滤指定端口范围内的流量":"抓取 21 到 23 区间所有端口的流量：\ntcpdump portrange 21-23","过滤源地址目标地址#过滤源地址、目标地址":"只抓取源地址是 10.211.55.11 的包：\ntcpdump -i any src 10.211.55.11 只抓取目标地址为 10.211.55.11 的包：\ntcpdump -i any dst 10.211.55.11","过滤端口#过滤端口":"只抓取某个端口的数据包，比如查看 80 端口的数据包：\ntcpdump -i any port 80 只想抓取目标端口为 80 的数据包，也就是 80 端口收到的包，可以加上 dst：\ntcpdump -i any dst port 80","运算符#运算符":"tcpdump 可以用布尔运算符 and（\u0026\u0026）、or（||）、not（!）来组合出任意复杂的过滤器。\n# 抓取 ip 为 10.211.55.10 并且目的端口为 3306 的数据包 tcpdump -i any host 10.211.55.10 and dst port 3306 # 抓取源 ip 为 10.211.55.10，目标端口除了 22 以外所有的流量 tcpdump -i any src 10.211.55.10 and not dst port 22","运维工作流#运维工作流":"# 1. 使用 mtr 进行初步诊断，定位问题范围 # 使用 --tcp 和 -p 443 是为了模拟真实的 HTTPS 流量，更有可能穿透防火墙 # 观察几十秒到一分钟 # 发现：在第 8 跳（某个国际出口路由器）之后，Loss% 上升到 15%，Wrst 延迟超过 500ms。 mtr -n --tcp -p 443 example.com # 2. 使用 traceroute 快速确认路径 traceroute -n -p 443 example.com # 3. 得出结论并提供证据 # 问题出在出国链路的第 8 跳节点附近，存在严重丢包和高延迟。 # 保存 mtr 报告截图或文本 mtr 是绝大多数情况下的首选。当你需要诊断网络慢、抖动、断线等质量问题时，优先使用 mtr。它能提供持续的数据，让你清晰看到丢包和延迟发生在哪一跳。 当你只需要快速看一眼路由路径是否正常，或者在脚本中调用时，使用 traceroute。它的输出更简单，更适合自动化处理。","限制包大小#限制包大小":"当包体很大，可以用 -s 截取部分报文内容，一般和 -A 一起使用。\n# 查看每个包体前 500 字节 tcpdump -i any -nn port 80 -A -s 500 显示包体所有内容，可以加上 -s 0。"},"title":"网络"},"/devops-learn/docs/linux/09_git/":{"data":{"":"","gh#gh":"gh 是 GitHub CLI（命令行工具），可以直接在命令行中执行常见的 GitHub 操作（如创建仓库、PR、Issue、Review、Actions 等），非常适合 DevOps 或日常运维自动化使用。","git#git":"Git 命令的一些使用技巧。","issue-管理#Issue 管理":"# 创建 Issue gh issue create --title \"API bug\" --body \"Detail about bug\" # 查看 Issue 列表 gh issue list # 查看 Issue 详情 gh issue view 42 gh issue view 42 --web # 关闭 Issue gh issue close 42 # 重新打开 Issue gh issue reopen 42 Actions：\n# 查看最近的 workflow 运行 gh run list # 查看运行详情 gh run view 123456789 # 打开 workflow 运行网页 gh run view 123456789 --web # 重新运行 workflow gh run rerun 123456789 # 取消运行 gh run cancel 123456789 # 下载 workflow 日志 gh run download 123456789","pull-request-管理#Pull Request 管理":"# 创建 PR（当前分支 -\u003e main） gh pr create --base main --title \"Fix bug\" --body \"Bug details...\" # 查看 PR 列表 gh pr list # 查看单个 PR 详情 gh pr view 123 gh pr view 123 --web # 打开网页查看 # 合并 PR gh pr merge 123 # 默认创建 merge commit gh pr merge 123 --squash # squash 合并 gh pr merge 123 --rebase # rebase 合并 # 检出某个 PR（拉取分支进行测试） gh pr checkout 123 # 关闭 PR gh pr close 123","release-与-tag-管理#Release 与 Tag 管理":"# 创建 release gh release create v1.0.0 --title \"v1.0.0\" --notes \"Initial release\" # 附加文件 gh release create v1.0.0 ./build/app.tar.gz # 查看 release 列表 gh release list # 查看 release 详情 gh release view v1.0.0 # 删除 release gh release delete v1.0.0","仓库管理#仓库管理":"# 创建一个新仓库（本地 + 远程） gh repo create my-repo --public # 只在 GitHub 上创建仓库 gh repo create my-repo --public --confirm --source=. # 克隆仓库（替代 git clone） gh repo clone user/repo # 打开当前仓库的 GitHub 页面 gh repo view --web # 查看仓库信息 gh repo view user/repo # Fork 仓库 gh repo fork user/repo # 删除远程仓库 gh repo delete user/repo","安装与认证#安装与认证":"# 安装（macOS） brew install gh # 登录 GitHub（支持 Web 登录或 Token） gh auth login # 查看当前登录信息 gh auth status # 退出登录 gh auth logout","常见运维场景#常见运维场景":"自动重跑失败的 Actions：\ngh run list --limit 10 --json databaseId,status | jq -r '.[] | select(.status==\"failure\") | .databaseId' | xargs -n1 gh run rerun 快速打开 PR 或 Issue：\ngh pr view --web gh issue view 101 --web"},"title":"Git"},"/devops-learn/docs/linux/firewall/":{"data":{"":"防火墙分为两类：\n软件防火墙，CentOS 6 的默认防火墙是 iptables，CentOS 7 的默认防火墙是 firewalld，底层都是使用内核中的 netfilter实现的。 包过滤防火墙，主要用于数据包的过滤，数据包转发。 应用层防火墙，可以控制应用程序的具体的行为。 硬件防火墙，例如 Cisco ASA、 Juniper SRX 等。"},"title":"防火墻"},"/devops-learn/docs/linux/firewall/01_iptables/":{"data":{"":"iptables 的核心工作原理可以概括为：“根据规则，对数据包进行过滤或处理”。这些规则被组织在预定义的链（Chains） 和表（Tables） 中。\nTables 由 Chains 组成，而 Chains 又由规则（Rules）组成。","probability#probability":"iptables 的 statistic 模块支持基于概率的规则匹配，通过 --mode random 和 --probability 参数实现随机匹配。此功能常用于负载均衡或流量分配。\niptables -A PREROUTING -t nat -p tcp -d 192.168.1.1 --dport 27017 \\ -m statistic --mode random --probability 0.33 \\ -j DNAT --to-destination 10.0.0.2:1234 iptables -A PREROUTING -t nat -p tcp -d 192.168.1.1 --dport 27017 \\ -m statistic --mode random --probability 0.5 \\ -j DNAT --to-destination 10.0.0.3:1234 iptables -A PREROUTING -t nat -p tcp -d 192.168.1.1 --dport 27017 \\ -j DNAT --to-destination 10.0.0.4:1234 第一条规则有 33% 的概率命中。 第二条规则有 50% × (1 − 33%) = 33% 的概率命中。 第三条规则没有指定 --probability，因此剩余的 34% 流量会命中。","使用场景#使用场景":"# 1. 设置默认策略（最严格的策略） iptables -P INPUT DROP # 默认拒绝所有入站 iptables -P FORWARD DROP # 默认拒绝所有转发 iptables -P OUTPUT ACCEPT # 默认允许所有出站（通常这样设置） # 2. 允许本地回环接口（localhost通信） iptables -A INPUT -i lo -j ACCEPT # 3. 允许已建立的和相关连接通过（关键！否则无法收到响应包） iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT # 4. 允许ICMP（ping命令） iptables -A INPUT -p icmp -j ACCEPT # 5. 开放特定服务端口（按需添加） iptables -A INPUT -p tcp --dport 22 -j ACCEPT # SSH iptables -A INPUT -p tcp --dport 80 -j ACCEPT # HTTP iptables -A INPUT -p tcp --dport 443 -j ACCEPT # HTTPS # 进入防火墙的数据包目的地址转换，从网口 eth0 进入的数据包，把目的 IP 为 114.115.116.117，端口为 80 的数据包，转到 10.0.0.1 # 这里外网用户访问公网地址 114.115.116.117:80，防火墙再转发到内网地址 iptables -t nat -A PREROUTING -i eth0 -d 114.115.116.117 -p tcp --dport 80 -j DNAT --to-destination 10.0.0.1 # 源地址转换，源地址 10.0.0.0/24 ，从网口 eth1 发出，并把源地址伪装成 111.112.113.114，响应回来后再转换为源地址 # 这里是内网地址 10.0.0.0/24 主机访问外网，会将内网地址伪装成公网 IP 111.112.113.114 iptables -t nat -A POSTROUTING -s 10.0.0.0/24 -i eth1 -j SNAT --to-source 111.112.113.114 规则的顺序问题：\n规则的顺序很重要，先匹配的规则先执行。 如果没有匹配的规则，那么就会使用默认策略。 # 可以接收从 IP 为 10.0.0.1 发送的数据包 iptables -t filter -A INPUT -s 10.0.0.1 -j ACCEPT iptables -A INPUT -s 10.0.0.2 -j ACCEPT iptables -A INPUT -s 10.0.0.2 -j DROP INPUT 链配置了两条规则：\n分别是接收 IP 为 10.0.0.2 的数据包； 和丢弃 IP 为 10.0.0.2 的数据包。 那么 10.0.0.2 的数据包能不能进来？\n可以。数据包会先匹配前面的 ACCEPT 10.0.0.2 的规则，这个时候数据包就进入了系统，所以规则顺序很重要。可以使用 -I 把规则从头插入：\niptables -I INPUT -s 10.0.0.2 -j ACCEPT","命令#命令":"命令格式：iptables [-t 表] 命令选项 [规则链] 规则\n-t 选项默认使用的是 filter 表。","命令选项#命令选项":"-A (Append)\t在链的末尾追加一条新规则。 例如：iptables -A INPUT -s 192.168.1.100 -j DROP 表示在 INPUT 链的末尾追加一条规则，当源 IP 是 192.168.1.100 时，拒绝该数据包。 -I (Insert)\t在链的指定位置插入一条新规则（默认为第1条）。 例如：iptables -I INPUT 3 -p tcp --dport 80 -j ACCEPT 表示在 INPUT 链的第 3 条位置插入一条规则，当协议是 TCP 且目标端口是 80 时，接受该数据包。 -D (Delete)\t从链中删除一条规则。 例如：iptables -D INPUT -s 192.168.1.100 -j DROP 表示从 INPUT 链中删除一条规则，当源 IP 是 192.168.1.100 时，拒绝该数据包。 -F (Flush) 清空指定链（或所有链）的所有规则。 例如：iptables -F INPUT 表示清空 INPUT 链的所有规则。 -L (List) 列出指定链（或所有链）的所有规则。 例如：iptables -L -n -v 表示列出所有链的所有规则，且不显示 IP 地址和端口号，而是显示数字形式的 IP 地址和端口号。 -N (New) 创建一条新的用户自定义链。 例如：iptables -N CUSTOM_CHAIN 表示创建一条名为 CUSTOM_CHAIN 的用户自定义链。 -X (Delete chain) 删除一条用户自定义链。 例如：iptables -X CUSTOM_CHAIN 表示删除名为 CUSTOM_CHAIN 的用户自定义链。 -P (Policy)\t设置链的默认策略（所有规则都不匹配时执行的动作）。 例如：iptables -P INPUT DROP 表示设置 INPUT 链的默认策略为 DROP，即所有不匹配的数据包都被拒绝。","小结#小结":"表和链的关系：你可以想象数据包流经一条“链”时，会依次经过挂在这条链上的不同“表”的规则检查。\n表的优先级顺序决定了检查的先后次序：raw -\u003e mangle -\u003e nat -\u003e filter。","工作流程数据包的一生#工作流程：数据包的一生":"入口 (PREROUTING)： 数据包从网卡进入系统。 首先经过 PREROUTING 链。这里会依次应用 raw、mangle、nat 表中的规则。 关键：在 nat 表的 PREROUTING 链中，可以做 DNAT（目标地址转换），比如把访问公网IP 80 端口的数据包转发到内网服务器的 192.168.1.10:80。 路由判断 (Routing Decision)： 内核查看数据包的目标 IP 地址，决定这个包是发给本机的（走 INPUT 链）还是需要转发的（走 FORWARD 链）。 发给本机 (INPUT)： 数据包进入 INPUT 链。这里会依次应用 mangle 和 filter 表中的规则。 关键：在 filter 表的 INPUT 链中，设置防火墙规则的主要地方。比如只允许特定 IP 访问本机的 SSH 端口。 本机进程处理： 数据包被本机的用户进程（如 web 服务器、SSH 服务）接收和处理。 本机发出 (OUTPUT)： 本机进程产生新的数据包，准备发送出去。 数据包进入 OUTPUT 链。这里会依次应用 raw、mangle、nat、filter 表中的规则。 关键：在 filter 表的 OUTPUT 链中，可以控制本机能发出哪些数据包。 转发 (FORWARD)： 如果数据包是转发的（第2步判断），则进入 FORWARD 链。 这里会依次应用 mangle 和 filter 表中的规则。 关键：在 filter 表的 FORWARD 链中，设置转发规则的主要地方。比如允许内网网段访问互联网，但禁止两个内网网段之间互访。 出口 (POSTROUTING)： 所有即将从网卡发出的数据包（无论是本机产生的还是转发的），最后都要经过 POSTROUTING 链。 这里会依次应用 mangle 和 nat 表中的规则。 关键：在 nat 表的 POSTROUTING 链中，可以做 SNAT（源地址转换），也就是常说的“IP伪装”（Masquerading），让内网机器共享一个公网IP上网。 离开：数据包离开网卡，前往下一个目的地。","核心思想#核心思想":"","表#表":"系统预定义了五个表，但最常用的是前两个：\nfilter 表：负责过滤数据包，决定是否放行。这是最常用的表。 内置链：INPUT, FORWARD, OUTPUT。 nat 表：负责网络地址转换（NAT）。 内置链：PREROUTING (DNAT), OUTPUT, POSTROUTING (SNAT)。 mangle 表：负责修改数据包的头信息（如 TTL、TOS）。 内置链：所有五个链 (PREROUTING, INPUT, FORWARD, OUTPUT, POSTROUTING)。 raw 表：负责连接跟踪机制的处理（如决定是否对数据包进行状态跟踪）。 内置链：PREROUTING, OUTPUT。 security 表（较少用）：用于强制访问控制（MAC）网络规则。","规则匹配与动作target#规则匹配与动作（Target）":"每条规则都由两部分组成：匹配条件（Matches） 和 动作（Target）。\n匹配条件：例如 -p tcp --dport 22（协议是 TCP 且目标端口是 22）、-s 192.168.1.100（源 IP 是 192.168.1.100）。 动作（Target）：当数据包匹配规则后要执行的操作。常见的有： ACCEPT：接受数据包，允许其通过。 DROP：丢弃数据包，没有任何响应。就像对方从来没发过这个包一样。更安全。 REJECT：拒绝数据包，并向发送方返回一个 connection refused 的错误消息。更友好。 SNAT：在 nat 表中使用，修改源地址。 DNAT：在 nat 表中使用，修改目标地址。 MASQUERADE：是 SNAT 的一种特殊形式，适用于动态获取 IP 的场合（如拨号上网）。 LOG：将匹配的数据包信息记录到系统日志（/var/log/messages 等），然后继续匹配后续规则。用于调试。","规则选项#规则选项":"-i 输入网口 -o 输出网口 -p 匹配网络协议：tcp、udp、icmp --icmp-type type 匹配 ICMP 类型，和 -p icmp 配合使用。 -s 匹配来源主机（或网络）的IP地址 --sport port 匹配来源主机的端口，和 -s source-ip 配合使用。 -d 匹配目标主机的 IP 地址 --dport port 匹配目标主机（或网络）的端口，和 -d dest-ip 配合使用。 -j 动作：指定规则匹配后要执行的动作。 例如：-j ACCEPT 表示接受该数据包。 例如：-j DROP 表示拒绝该数据包。 例如：-j LOG 表示记录该数据包的信息到系统日志。 例如：-j SNAT --to-source 192.168.1.100 表示修改源地址为 192.168.1.100。 例如：-j DNAT --to-destination 192.168.1.100 表示修改目标地址为 192.168.1.100。 例如：-j CUSTOM_CHAIN 表示跳转到名为 CUSTOM_CHAIN 的用户自定义链。 例如：-j MASQUERADE 表示使用动态获取的 IP 地址进行 SNAT。","链#链":"链是规则的集合，这些规则按顺序排列。数据包到达某个链时，会从第一条规则开始依次匹配，一旦匹配成功，就执行该规则定义的动作（如放行、拒绝），并停止后续匹配。如果所有规则都不匹配，则执行该链的默认策略（Policy）。\n系统预定义了五个最重要的链（对应数据包流经的不同阶段）：：\nINPUT：处理本机接收的数据包（例如，有人 ping 你的机器或 SSH 连接到你的机器）。 OUTPUT：处理本机发出的数据包（例如，你从本机 ping 别人）。 FORWARD：处理经过本机路由的数据包（你的机器充当路由器或网关时）。 PREROUTING：(nat 表) 数据包刚到达防火墙，在进行路由判断之前（可用于修改目标地址，即 DNAT）。 POSTROUTING：(nat表) 数据包即将离开防火墙，在进行路由判断之后（可用于修改源地址，即 SNAT）。"},"title":"iptables"},"/devops-learn/docs/terraform/":{"data":{"":"","什么是-iac#什么是 IaC":"IaC 是 Infrastructure as Code 的缩写，即基础设施即代码。\n基础设施及代码是一种方法，通过代码来定义和管理基础设施，而不是通过手动操作来完成。\nIaC 工具：\nAzure：ARM Template 和 Bicep GCP：Google Cloud Deployment Manager AWS：CloudFormation 全平台：Terraform 这些 IaC 工具采用了完全声明式模型。用户不再关注过程步骤，而是在配置文件中定义了他们期望的基础设施状态。\n这些工具将这种状态与现实进行协调，自动执行实现结果所需的行动。Terraform 引入了状态文件来跟踪资源，从而实现增量更新和可扩展性，而 CloudFormation 利用 JSON 或 YAML 模板以声明方式管理 AWS 资源。两者都为命令式模型带来的挑战提供了独特的解决方案。","声明式和命令式#声明式和命令式":"基础设施即代码（IaC）的演进，历经了从命令式到声明式的重大转变。\n在早期的命令式配置管理阶段，Chef和Puppet等工具的崛起，标志着基础设施配置自动化的新篇章。这些平台通过明确地概述实现所需配置的步骤，为用户提供了一种全新的配置系统方法。然而，随着业务需求的不断变化和技术的飞速发展，单纯的命令式管理已难以满足日益复杂的环境需求。\n声明式 IaC 应运而生。它打破了传统的命令式束缚。让用户能够更专注于描述目标状态，而非具体实现细节。\n声明式的优点：\n幂等性：无论执行多少次，都能得到相同的结果。不会导致多次创建或者破坏其他资源。 简单，减少出错：用户不必知道当前设置的状态，不需要指定如何从当前状态转变到目标状态。只需要定义所期望的状态，Terraform 会自动计算出当前状态与期望状态的差异，并执行必要的操作。 文档化：配置文件本身就是基础设施的最新、最准确的文档。 版本控制与可追溯性：基础设施代码可以像应用代码一样进行版本管理（Git），方便审查、回滚和审计。"},"title":"Terraform"},"/devops-learn/docs/terraform/01_terraform/":{"data":{"":"","aws#AWS":"静态凭证 (Static Credentials) - 不推荐用于生产 硬编码 (极不推荐，最不安全)，会被提交到 Git 仓库：\nprovider \"aws\" { region = \"us-west-2\" access_key = \"my-access-key\" secret_key = \"my-secret-key\" } 环境变量：\nexport AWS_ACCESS_KEY_ID=\"anaccesskey\" export AWS_SECRET_ACCESS_KEY=\"asecretkey\" export AWS_REGION=\"us-west-2\" terraform plan provider \"aws\" {} 共享凭证文件：\nexport 环境变量的方式只在当前 Terminal 生效，关闭 Terminal 后就失效了。想要全局生效，使用凭证文件 ~/.aws/credentials。\nprovider \"aws\" { region = \"us-west-2\" } 凭证文件：\n当你运行 aws configure 后，凭证会保存在 ~/.aws/credentials 中。Terraform 会自动读取这个文件。\n# ~/.aws/credentials [default] aws_access_key_id = AKIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY 对应的 ~/.aws/config 文件定义区域：\n# ~/.aws/config [default] region = us-east-1 避免使用静态凭证，尤其是硬编码。如果必须使用，请仅用于个人测试账户，并确保 terraform 文件绝不包含凭证并提交到 Git。\nIAM 角色和临时安全凭证 (IAM Roles \u0026 Temporary Security Credentials) 这是 AWS 和 Terraform 推荐的最佳实践，因为它提供了更高的安全性。凭证是临时的（默认最多 1 小时），过期后自动失效，无需轮换。\n通过环境变量提供临时凭证：\n临时凭证除了 Access Key 和 Secret Key，还有一个 Session Token。\nexport AWS_ACCESS_KEY_ID=\"ASIAIOSFODNN7EXAMPLE\" export AWS_SECRET_ACCESS_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\" export AWS_SESSION_TOKEN=\"your-very-long-session-token\" terraform plan 在共享凭证文件中配置临时凭证：\n在 ~/.aws/credentials 中，可以为一个 Profile 配置临时凭证：\n[default] aws_access_key_id = ASIAIOSFODNN7EXAMPLE aws_secret_access_key = wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY aws_session_token = your-very-long-session-token IAM 实例配置文件 当 Terraform 运行在 AWS 资源内部时（例如在 EC2 实例上），这是最安全、最推荐的方式。\n原理：\n你创建一个 IAM 角色（Role）并授予它必要的权限。 你将这个角色附加（Attach）到 EC2 实例上（通过实例配置文件 Instance Profile）。 EC2 实例上的应用程序（包括 Terraform）可以通过内网的实例元数据服务 (IMDS) 自动获取该角色的临时安全凭证。 Terraform AWS Provider 会自动发现并使用这些凭证，你无需做任何配置。","backend#backend":"backend 是 Terraform 用来存储状态文件的地方。","data-sources-示例#Data Sources 示例":"Provider 的文档中，不仅有 Resource，还有 Data Source。例如 EC2 目录下有 Resources，还有 Data Sources。\ndata \"aws_vpc\" \"exist_vpc\" { # 这里可以定义的过滤条件，告诉 aws，按照什么条件去查找，输出一个符合条件的 VPC # filter {} default = true # 表示如果没有找到符合条件的 VPC，就使用默认的 VPC } resource \"aws_subnet\" \"dev_subnet\" { vpc_id = data.aws_vpc.exist_vpc.id cidr_block = \"10.0.1.0/24\" # 不能和默认的 VPC 下的子网的 CIDR 冲突 availability_zone = \"us-east-1a\" }","for-循环示例#For 循环示例":"resource \"azurerm_resource_group\" \"resourcegroup\" { name = var.resource_group_name location = var.location # 引用变量，使用 var.\u003c变量名\u003e } resource \"azurerm_virtual_network\" \"vnet\" { name = \"ExampleVNet\" address_space = [\"10.0.0.0/16\"] location = azurerm_resource_group.resourcegroup.location resource_group_name = azurerm_resource_group.resourcegroup.name # 不使用 var.resource_group_name，因为创建顺序是无序的， # 直接使用，可能会报错，因为 vnet 依赖 resource group，而 resource group 没有创建完成，var.resource_group_name 这个 resource group 还不存在 } resource \"azurerm_subnet\" \"subnet\" { for_each = var.subnets # for each 变量 subnets，遍历里面每一个 instance resource_group_name = azurerm_resource_group.resourcegroup.name virtual_network_name = azurerm_virtual_network.vnet.name name = each.value[\"name\"] # 提取变量 subnets 中的一个元素的 name 的值 address_prefixes = each.value[\"address_prefixes\"] # 提取变量 subnets 中的一个元素的 address_prefixes 的值 } variables.tf：\nvariable \"subnets\" { type = map(any) default = { subnet_1 = { name = \"Subnet1\" address_prefixes = [\"10.0.0.0/24\"] # []代表列表，一个或多个值 } subnet_2 = { name = \"Subnet2\" address_prefixes = [\"10.0.1.0/24\"] } subnet_3 = { name = \"Subnet3\" address_prefixes = [\"10.0.2.0/24\"] } bastion_subnet = { name = \"AzureBastionSubnet\" # bastion 的 subnet 必须是这个名字 address_prefixes = [\"10.0.250.0/24\"] } } }","module-示例#Module 示例":"模块用来存储可复用的代码，如果在每一个 terraform 项目中都写一遍相同的代码，会导致代码重复。这个时候就可以使用模块，\nModule 是有层级的，例如：\nterraformpro ├── main.tf ├── variables.tf ├── outputs.tf ├── terraform.tfvars ├── modules │ ├── module1 │ │ ├── main.tf │ │ ├── variables.tf │ │ ├── outputs.tf │ │ ├── terraform.tfvars │ ├── module2 │ ├── module3 main.tf：\n# call module1 which has 2 variables module \"module1\" { source = \"./modules/module1\" # 指定模块的路径 basename = \"examplemodule1\" # 设置模块变量的值 location = \"West Europe\" # 设置模块变量的值 } # call module2 which has 3 variables module \"module2\" { source = \"./modules/module2\" basename = \"examplemodule2\" # resource_group_name 是 module1 创建完以后返回的，虽然我们可以知道 module1 创建的 resource group 名称是 ExampleRG # 但是在 module2 中并不想写死 # module1 的 output 文件输出了 resource_group_name resource_group_name = module.module1.resource_group_name location = \"West Europe\" } module1/outputs.tf：\n# 模块输出 resource_group_name output \"resource_group_name\" { # \u003c资源类型\u003e.\u003c资源名称\u003e. value = azurerm_resource_group.resourcegroup.name } 一个 output 属性只能定义一个 value。","state-命令#state 命令":"terraform state 命令用于查看和管理 Terraform 状态文件。\nterraform state list：列出状态文件中的所有资源。 terraform state show ：显示指定资源的详细信息。这在调试、查找特定信息（如 IP 地址、ARN）时极其有用。 terraform state rm ：从状态文件中删除指定资源。但不会销毁实际的云资源。 如果有人绕过 Terraform 在控制台上删除了资源，状态文件中就会留下一个“幽灵”记录。terraform plan 会报错说资源找不到。这时可以用 state rm 来清理这个无效记录。 terraform state rm aws_instance.web Terraform 会“忘记”它曾管理过这个 EC2 实例，但该实例仍在 AWS 中运行。下次 apply 时，Terraform 会试图创建一个新的 aws_instance.web（如果配置还在），导致资源冲突。 terraform state pull：从远程后端拉取最新状态。terraform state pull \u003e state.json 主要用于编程式处理状态文件内容，或者用于调试复杂问题。 terraform state push：将本地状态推送到远程后端。 terraform state mv ：移动资源在状态文件中的位置。不会影响真实的基础设施。云上的资源完好无损，只是 Terraform 管理它的“名字”变了。你必须提前在代码中配置好目标资源块。这个命令只修改状态，不修改代码。","terraformtfstate-什么时候更新#terraform.tfstate 什么时候更新":"成功执行 terraform apply 或 terraform destroy 命令后更新。","terraformtfstatebackup#terraform.tfstate.backup":"terraform.tfstate.backup 是一个备份文件，是 Terraform 在覆盖当前状态文件 (terraform.tfstate) 之前自动创建的备份副本。它的核心作用是充当“后悔药”，让你在操作失败或出现意外时，能够恢复到操作前的已知状态。\nterraform.tfstate.backup 会在你执行任何会修改状态文件的 Terraform 命令时生成。具体来说，当以下两个条件同时满足时：\n执行了会修改状态文件的操作： terraform apply（创建、更新或销毁资源） terraform destroy terraform refresh （替代品 apply -refresh-only 也会） 使用 -target 参数的上述命令 terraform import 使用本地后端 (local backend)：即你的状态文件 (terraform.tfstate) 存储在本地磁盘上。如果你使用了远程后端（如 S3、Azure Storage），Terraform 通常不会在本地生成 .backup 文件，因为备份和版本控制由后端自己处理（例如 S3 的对象版本控制）。","variable-示例#Variable 示例":"variables.tf：\nvariable \"location\" { type = string default = \"West Europe\" description = \"The location of the resource group.\" } variable \"storage_account_name\" { type = string description = \"The name of the storage account.\" # 没有默认值，因为每个 storage account 名称在 azure 中是全局唯一的。 } main.tf：\n# locals 也是一个变量，本地变量 # 本地变量可存储重复出现的值或表达式，例如多次引用的配置信息。通过 locals 块声明后，可避免在多个地方重复编写相同内容 # locals 的作用域是模块级别的 # - 目录作用域：在同一个 Terraform 模块目录（包含所有 .tf 文件的那个目录）中，任何地方定义的 locals 块在整个模块中都可用 # - 全局可见：一旦在某个 locals 块中定义了一个值（例如 local.name_prefix），你可以在同一个模块的任何 .tf 文件中通过 local.name_prefix 引用它 # - 不可跨模块：locals 对于父模块或子模块是不可见的。如果需要在模块间共享值，应该使用 output。 # 引用 locals 变量时，直接使用 local.\u003c变量名\u003e locals { tags = { usage = \"test\" owner = \"sid\" } } resource \"azurerm_resource_group\" \"resourcegroup\" { # name = \"${var.prefix}-RG\" 这是另一种变量的用法，可以将 var.prefix 和 -RG 组成一个字符串 name = \"ExampleRG\" location = var.location # 引用变量，使用 var.\u003c变量名\u003e } resource \"azurerm_storage_account\" \"storageaccount\" { name = var.storage_account_name resource_group_name = azurerm_resource_group.resourcegroup.name # 引用资源，使用 \u003c资源类型\u003e.\u003c资源名称\u003e. location = azurerm_resource_group.resourcegroup.location # 这种方式表示在 resourcegroup 创建完了以后使用 resourcegroup 的 name/location account_tier = \"Standard\" account_replication_type = \"LRS\" tags = local.tags } storage_account_name 没有默认值，所以肯定需要指定一个值。有两种方式：\n在 terraform apply 时，会提示输入 storage_account_name 的值。 在 terraform plan/apply 时，需要添加 -var \"storage_account_name=mystorageaccount\" 来指定这个值。不方便。 创建 terraform.tfvars 文件，在文件中指定变量的值。最推荐的方式。 环境变量 terraform.tfvars 文件：\nstorage_account_name = \"mystorageaccount\"","variable-类型#Variable 类型":"string number bool list()：列表，例如 list(string) set()：集合，例如 set(string) map()：映射对象，例如 map(string) object({ = , ... })：对象，例如 object({ name = string, age = number }) tuple([, ...])：元组，例如 tuple([string, number, bool]) variable \"image_id\" { type = string } variable \"availability_zone_names\" { type = list(string) default = [\"us-west-1a\"] } variable \"docker_ports\" { type = list(object({ internal = number external = number protocol = string })) default = [ { internal = 8300 external = 8300 protocol = \"tcp\" } ] }","workspace#workspace":"Workspaces 隔离的是 State，而不是代码或变量。快速、轻量地创建配置相同但状态隔离的环境。\n它非常适用于短期存在的环境，如功能分支开发环境、临时测试环境，或者需要快速复制一套完整基础设施的场景。","什么时候应该避免使用-workspaces#什么时候应该避免使用 Workspaces？":"对于严格隔离的生产环境（Production），最佳实践通常不是使用 Workspaces，而是使用以下更强大的隔离方式：\n独立的 Terraform 配置目录：为 prod 和 dev 创建完全独立的代码目录。这提供了最强的隔离性。\n不同的版本控制分支：main 分支代表生产状态，dev 分支用于开发。\n不同的云账户（AWS Account / Azure Subscription）：这是黄金标准。通过物理账户边界实现绝对的权限和资源隔离。使用 Terraform 云提供商别名或不同的后端来管理不同账户的资源。\n不同的 Terraform 后端：为每个环境使用完全独立的 S3 桶和 DynamoDB 表来存储状态。","使用场景#使用场景":"你有一套定义好的基础设施代码（例如：1个VPC、2个EC2实例、1个RDS数据库）。现在你需要为功能开发、测试和预发布各部署一套完全一样但完全隔离的环境。\n# 创建新工作区 terraform workspace new feature-login terraform workspace new staging terraform workspace new preprod # 切换工作区并部署 terraform workspace select feature-login terraform apply -var-file=feature.tfvars terraform workspace select staging terraform apply -var-file=staging.tfvars terraform workspace select preprod terraform apply -var-file=preprod.tfvars","使用已经存在的-module#使用已经存在的 module":"module \"vpc\" { source = \"terraform-aws-modules/vpc/aws\" name = \"my-vpc\" cidr = \"10.0.0.0/16\" azs = [\"eu-west-1a\", \"eu-west-1b\", \"eu-west-1c\"] private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\", \"10.0.3.0/24\"] public_subnets = [\"10.0.101.0/24\", \"10.0.102.0/24\", \"10.0.103.0/24\"] enable_nat_gateway = true enable_vpn_gateway = true tags = { Terraform = \"true\" Environment = \"dev\" } }","修复一个被手动修改的资源#修复一个被手动修改的资源":"发现问题：terraform plan 报告说有漂移（drift），比如安全组的规则被人在 AWS 控制台上改了。 先使用 terraform state show 查看资源的当前状态。 查看真实状态：去 AWS 控制台确认安全组的真实规则。 修复 drift：根据 AWS 控制台的真实规则，修改 Terraform 配置文件。 执行 terraform apply 应用变更。 确认资源已被正确创建或更新。","其他常用命令#其他常用命令":"","参数#参数":"HCL 中的参数就是将一个值赋给一个特定的名称：\nname = \"example\" 等号前的标识符就是参数名，等号后的表达式就是参数值。","基本工作流程#基本工作流程":"Init 初始化，初始化工作目录。下载配置中声明的 Provider 插件、模块等依赖项。 terraform init，第一次使用新配置时，或添加/修改了 Provider 或模块后。 Plan 计划，它会对比状态文件（当前状态）和配置文件（期望状态），然后生成一个执行计划，算出来需要执行的步骤。 terraform plan 在真正应用变更之前进行审查和确认，避免意外操作。 Apply 应用，会根据执行计划，来执行你的操作。 terraform apply，执行计划中的变更，使现实基础设施的状态与你的配置保持一致。 你可以在 apply 时添加 -auto-approve 参数，来自动 approve 这个计划。否则会提示你是否 approve 这个计划。 你也可以在 apply 时添加 -target 参数，来指定只执行某个资源。 你也可以在 apply 时添加 -destroy 参数，来指定只删除某个资源。 Destroy 销毁，会销毁所有资源。也需要确认是否销毁。支持 -auto-approve。 terraform destroy，清理环境，避免产生不必要的费用。 删除资源可以直接在配置文件中删除配置，然后执行 terraform apply 来删除资源（推荐）。 也可以使用 terraform destroy -target \u003c资源类型\u003e.\u003c资源名称\u003e 来删除指定资源，不推荐，因为会导致状态与配置文件不一致。所有的更改都应该通过配置文件来进行。","多个环境#多个环境":"对于多个环境，每个环境的变量值可能会不同。\n例如：\n开发环境 测试环境 生产环境 每个环境的变量值可能会不同，每个环境都创建一个 terraform.tfvars 文件。例如：\n开发环境：terraform-dev.tfvars 测试环境：terraform-test.tfvars 生产环境：terraform-prod.tfvars apply 时，需要添加 -var-file 来指定 terraform.tfvars 文件的路径。例如：\nterraform apply -var-file=terraform.dev.tfvars","失败#失败":"apply 失败会生成 crash.log 文件, 可以根据这个文件来调试.","如何在工作区之间实现差异配置#如何在工作区之间实现差异配置？":"使用输入变量和条件表达式。\n使用不同的变量文件（.tfvars）（推荐）：\nterraform workspace select dev terraform apply -var-file=\"dev.tfvars\" terraform workspace select prod terraform apply -var-file=\"prod.tfvars\"","它有什么用#它有什么用？":"这是一个灾难恢复机制。\n操作失败或中断： apply 操作在执行中途失败（如网络中断、权限突然失效、配额不足）。 此时，部分资源可能已经被创建或修改，而另一部分没有。 结果：状态文件 (terraform.tfstate) 可能处于一个不完整、不一致或损坏的状态，因为它只记录了一部分变更。 意外后果： apply 操作成功完成了，但引入了一个你未曾预料到的严重问题（例如，错误地覆盖了一个关键配置）。 结果：状态文件是最新的，但基础设施的状态是错误的。 在这些情况下，可以检查 .backup 文件，了解操作开始前基础设施的状态。从而进行问题诊断。","标识符#标识符":"合法的标识符可以包含字母、数字、下划线(_)以及连字符(-)。 标识符首字母不可以为数字。","核心配置块#核心配置块":"terraform 配置块：定义 Terraform 版本、插件源等。 required_providers：指定所需的 Provider 插件源及其版本。 backend：配置状态文件的存储后端，如本地文件、远程后端（如 S3 + DynamoDB）等。 provider 配置块 定义使用的 Provider 插件，如 AWS、Azure、GCP 等。 配置 Provider 插件的认证信息，如 API 密钥、访问令牌等。 resource 配置块 定义要创建的资源，如 AWS 实例、GCP 存储桶等。 配置资源的属性，如实例类型、存储桶名称等。 variable 配置块 定义输入变量，如输入参数、环境变量等。 配置变量的默认值，如 default = \"us-east-1\"。 output 配置块 定义输出值，应用完成后公开一些有用的信息（如服务器的 IP 地址）。 配置输出值的描述，如 description = \"The ID of the created instance\"。 配置输出值的敏感信息，如 sensitive = true，可以防止输出值被打印到日志中。 data 配置块 定义数据源，用于从提供商获取外部数据或查询已有资源的信息，以便在配置中使用。 配置数据源的属性，如实例 ID、存储桶名称等。 module 配置块 调用模块，将多个资源封装成一个可重用的单元。 配置模块的参数，对于 module 来说,输入变量就像函数的参数,输出值就像函数的返回值. output 用来导出资源的属性到父级模块 只有将一组资源组合在一起的 module 才有意义, 例如一组网络资源, 一组数据库资源等. 可以创建自己的模块,也可以使用其他存在的模块,例如 terraform 官方提供的模块 terraform-aws-modules/vpc locals 配置块 定义本地变量，用于在配置中重复使用的值。 配置本地变量的表达式，如 local_var = \"value\"。","概念#概念":"Providers (提供商) 是什么：Provider 是 Terraform 的插件，用于与特定的云平台（如 AWS, Azure, GCP）、 SaaS 服务（如 Cloudflare, Datadog）或本地基础设施（如 vSphere, Docker）进行交互。 作用：Provider 负责理解 API 交互并将资源创建、更新和删除的请求翻译成具体的 API 调用。 如何使用：在配置中通过 provider 块进行声明和配置（例如，设置 region 和 access key）。 示例：aws, azurerm, google, kubernetes, null。 Resources (资源) 是什么：Resource 是 Terraform 配置中最重要的元素，代表一个具体的基础设施资源。 作用：定义一个需要被创建、管理和销毁的资源实体，例如一台虚拟机、一个网络安全组、一个 S3 存储桶。 结构：resource \"resource_type\" \"resource_name\" { ... } resource_type：由 Provider 提供（如 aws_instance）。 resource_name：你在当前 Terraform 模块内给这个资源起的逻辑名称（标识符）。 示例：resource \"aws_instance\" \"my_web_server\" { ami = \"ami-12345\", instance_type = \"t2.micro\" } Input Variables (输入变量) 是什么：类似于函数的参数，用于从外部向 Terraform 模块传递值。 作用：参数化配置，提高代码的灵活性和可复用性。避免将敏感信息或环境特定的值硬编码在配置文件中。 如何定义：在 variables.tf 文件中使用 variable 块声明。 如何赋值：可以通过命令行 -var \"var_name=var_value\" 选项、.tfvars 文件、环境变量等方式传入。 Output Values (输出值) 是什么：类似于函数的返回值，用于将模块内部资源的信息暴露给外部。 作用：共享资源的属性（例如，新创建服务器的公有 IP 地址），以便其他 Terraform 配置或外部世界可以使用。 如何定义：在 outputs.tf 文件中使用 output 块声明。 State (状态) 是什么：一个名为 terraform.tfstate 的 JSON 文件，它极其重要。它存储了 Terraform 所管理基础设施的当前状态和属性映射。 作用： 映射现实：将你的配置文件（.tf）中的资源定义映射到现实世界中的真实资源 ID。 元数据存储：存储资源的依赖关系、属性等信息，用于计算增量变更。 性能：缓存资源信息，避免每次执行都需查询所有云资源。 重要提示：必须安全地存储和备份状态文件（例如使用远程后端如 S3 + DynamoDB），严禁手动修改。团队成员间应共享同一份状态文件以避免配置冲突。 Modules (模块) 是什么：将多个资源组合成一个更大、可重用单元的容器。一个模块就是一个包含 Terraform 配置文件（main.tf, variables.tf, outputs.tf）的目录。 作用：抽象和封装基础设施，实现代码复用和组织化。你可以创建自己的模块，也可以使用来自 Terraform Registry 的公共模块。 根模块：执行 terraform apply 时所在的目录。 子模块：在配置中通过 module 块调用的其他模块。 Data Sources (数据源) 是什么：数据源允许 Terraform 从外部数据源获取信息，而不是从本地配置文件中获取。比如现在要在一个现有的 aws VPC 中创建一个子网，而不是新的 VPC，如何获取 VPC id。Resource 用于创建和管理基础设施资源，Data Source 用于检索和利用现有基础设施资源的信息。 可以从控制台获取 VPC id，比较麻烦。 可以从数据源中获取 VPC id。 作用：检索和利用现有基础设施资源的信息，在配置中引用。 如何定义：在 data.tf 文件中使用 data 块声明。 示例：data \"aws_ami\" \"example\" { most_recent = true, owners = [\"self\"] } Resource 与 Data Source 可以看作为函数，Resource 的定义是一个创建资源的函数，然后传递参数给 Terraform，来创建对应的实体。Data Source 可以看作是一个查找并返回现有实体信息的函数，可以指定查询的条件和参数。","注释#注释":"# Configuration options 单行注释 // Configuration options 单行注释 /* 多行注释 Configuration options Configuration options */","状态#状态":"terraform 会将资源的状态保存在状态文件中。默认是 terraform.tfstate。这是一个 JSON 文件。\n每当更新资源后，terraform 会更新状态文件。\n如果删除了所有资源，那么 resources 会是一个空数组。","环境变量#环境变量":"可以通过设置名为 TF_VAR_ 的环境变量为输入变量赋值，例如：\n$ export TF_VAR_image_id=ami-abc123 $ terraform plan Terraform 要求环境变量中的 与 Terraform 代码中定义的输入变量名大小写完全一致。\nvariable \"image_id\" { type = string } 环境变量传值非常适合在自动化流水线中使用，尤其适合用来传递敏感数据，类似密码、访问密钥等。","示例#示例":"","简单示例#简单示例":"main.tf：\nterraform { required_providers { azurerm = { source = \"hashicorp/azurerm\" version = \"4.41.0\" } } } provider \"azurerm\" { # Configuration options # features 是必须的，可以为空 features {} } resource \"azurerm_resource_group\" \"example\" { name = \"ExampleRG\" location = \"West Europe\" } 运行 terraform init 初始化。会去搜索对应版本的 provider 插件下载，并安装到 .terraform 目录下。这个目录下会有一个可执行文件，就是 provider。\n.terraform.lock.hcl 是用来记录 provider 插件的版本信息的。包括 hash 值。\n如果已经运行了 az 登录，那么执行 terraform plan：\nterraform plan # 输出 Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols: + create Terraform will perform the following actions: # azurerm_resource_group.example will be created + resource \"azurerm_resource_group\" \"example\" { + id = (known after apply) + location = \"West Europe\" + name = \"ExampleRG\" } Plan: 1 to add, 0 to change, 0 to destroy. 执行 terraform apply 来创建资源。\n输出：\nTerraform will perform the following actions: # azurerm_resource_group.example will be created + resource \"azurerm_resource_group\" \"example\" { + id = (known after apply) + location = \"West Europe\" + name = \"ExampleRG\" } Plan: 1 to add, 0 to change, 0 to destroy. Do you want to perform these actions? Terraform will perform the actions described above. Only 'yes' will be accepted to approve. Enter a value: yes Apply complete! Resources: 1 added, 0 changed, 0 destroyed. 执行完以后会同时创建一个 terraform.tfstate 文件，用来记录所有被 terraform 管理的资源的状态。\nterraform 会使用这个状态文件去和 main.tf 中定义的资源进行对比，来判断是否需要创建、更新、删除资源。来达到预期的状态。","语法#语法":"","身份认证#身份认证":"如何证明你有权限在对应的云平台上操作。Terraform 本身不处理认证，而是委托给下载的 Provider 插件，并由插件遵循对应云平台的认证标准。\n云平台 认证方式 (推荐顺序) Terraform Provider 如何获取凭证 AWS 1. IAM 角色 (用于 EC2/ECS 等) 2. 共享凭证文件 (~/.aws/credentials) 3. 环境变量 4. 硬编码 (极不推荐) Provider 会使用 AWS SDK，自动按照标准 AWS CLI 的认证流程查找凭证。 Azure 1. 环境变量 (ARM_CLIENT_ID, ARM_CLIENT_SECRET, ARM_TENANT_ID, ARM_SUBSCRIPTION_ID) 2. CLI 认证 (az login) Provider 使用 Azure SDK，支持环境变量或自动继承已登录的 Azure CLI 的认证上下文。 Google Cloud 1. 应用默认凭证 (ADC) - gcloud auth application-default login 2. 服务账户密钥文件 3. 环境变量 Provider 使用 Google Cloud SDK，自动查找应用默认凭证或环境变量指定的密钥文件。 Kubernetes 1. 环境变量 (KUBERNETES_SERVICE_HOST, KUBERNETES_SERVICE_PORT) 2. 配置文件 (~/.kube/config) Provider 使用 Kubernetes 客户端库，自动查找环境变量或配置文件指定的集群信息。","运维工作流#运维工作流":"","远程后端#远程后端":"当团队多个人在使用时，每个人都会在本地执行 Terraform 命令, 或者正在将 Terraform 集成到 Jenkins 中。存在多个创建状态的地方.\n如何共享状态?\n远程后端存储.\nTerraform 引入了远程状态存储机制，也就是 Backend。Backend 是一种抽象的远程存储接口，如同 Provider 一样，Backend 也支持多种不同的远程存储服务：\nlocal remote azurerm consul cos gcs http kubernetes oss pg s3 状态锁是指，当针对一个 tfstate 进行变更操作时，可以针对该状态文件添加一把全局锁，确保同一时间只能有一个变更被执行。\n不同的 Backend 对状态锁的支持不尽相同，实现状态锁的机制也不尽相同，例如 consul Backend就通过一个 .lock 节点来充当锁。s3 Backend 则需要用户传入一个 Dynamodb 表来存放锁信息，而 tfstate 文件被存储在 S3 存储桶里。\nterraform { required_version = \"1.13.0\" backend \"s3\" { bucket = \"myapp-terraform-state-bucket\" key = \"myapp/terraform.tfstate\" region = \"eu-west-1\" } required_providers { aws = { source = \"hashicorp/aws\" version = \"~\u003e 4.0\" } } } 需要先去 aws 创建一个 s3 的 bucket. 名字是 my-terraform-state-bucket 然后在 bucket 中创建一个文件夹. 文件夹的名称就是 myapp/terraform.tfstate.\nbucket settings for Block Public Access:\nBlock all public access Bucket Versioning: bucket 版本控制,每次文件更改都会生成一个版本,可以回滚.\nEnable Default encryption: 开启默认加密, 可以防止数据泄露.\nDisable Create. 之后就创建了一个空桶","配置文件#配置文件":"Provider 部分 表示 provider 部分，用来指定你要使用的 provider，比如 aws，azure，gcp 等。\nterraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"6.10.0\" } } } provider \"aws\" { # Configuration options } 对于多个 provider 的情况，可以单独抽出一个 providers.tf 文件。\nterraform { required_providers { aws = { source = \"hashicorp/aws\" version = \"6.10.0\" } azurerm = { source = \"hashicorp/azurerm\" version = \"4.41.0\" } tencentcloud = { source = \"tencentcloudstack/tencentcloud\" version = \"1.123.0\" } } } Resource 部分 表示 resource 部分，用来指定你要创建的资源，比如 aws 中的 ec2，rds 等。\n# resource # \"aws_instance\" 表示资源类型 # \"example\" 表示资源名称，注意这个名字是你自己定义的，是 local 的，不是 aws 中的资源名称。 resource \"aws_instance\" \"example\" { name = \"example\" # aws 中的资源名称 ami = \"ami-0c55b159cbfafe1f0\" instance_type = \"t2.micro\" } Variable 文件 例如定义 resource 我们需要填一些参数，比如 ami，instance type 等。这些参数我们可以定义在 variable 文件中。在 variable 文件中定义的变量的值，然后在 resource 中引用，避免在 main.tf 中改来改去。\nOutput 文件 在有些情况在，只有当这个资源被创建出来之后，你才知道这个资源的一些信息，比如 id， ip 地址，dns 名称等。\n如果你想使用这些值，作为以下资源的输入参数，就可以使用 output 来导出。","重命名资源#重命名资源":"假设我们有一个简单的 EC2 实例资源：\n# main.tf (原始代码) resource \"aws_instance\" \"old_name\" { ami = \"ami-123456\" instance_type = \"t3.micro\" } 想把资源标识符从 old_name 改为 new_name。\n首先，修改代码。将 main.tf 中的资源块改名。resource \"aws_instance\" \"new_name\"。 执行 state mv 命令。告诉 Terraform 状态文件中原来的那个资源现在由新的名字来管理。 最后，验证。运行 terraform plan。输出应该显示 No changes. 这意味着基础设施不需要任何变更，只是状态文件的内部记录更新了。 如果直接改代码而不运行 state mv，Terraform 会计划销毁 aws_instance.old_name 并创建 aws_instance.new_name。","限制#限制":"它只备份前一个状态，对于完整的历史跟踪，要使用： 版本控制工具（如 Git），手动将 terraform.tfstate 和 .backup 文件提交到 Git，但这非常不推荐，因为状态文件包含敏感信息。 远程后端（如 S3、Azure Storage），最佳实践。每次状态更新都会在云端保存一个历史版本，你可以随时回滚到任何历史版本。 远程后端的行为不同：对于远程后端，本地不会生成 .backup，但后端本身提供了更强大的版本历史、状态锁定和恢复功能。应该优先使用远程后端。","限性和风险#限性和风险":"容易人为失误：很容易忘记切换工作区。你可能以为自己身在 dev，但实际上在 prod，一个 terraform destroy 就会导致灾难性后果。必须时刻使用 terraform workspace show 确认当前工作区。\n代码复杂性：在代码中嵌入大量 terraform.workspace 的逻辑会使配置变得复杂和难以理解，降低了代码的可读性和可维护性。"},"title":"Terraform"},"/devops-learn/docs/terraform/07_resource/":{"data":{"":"","元参数#元参数":""},"title":"Resource 资源"},"/devops-learn/docs/terraform/project/":{"data":{"":"如果 .tfvars 文件包含敏感数据，例如密码、访问密钥等，建议将其添加到 .gitignore 文件中，避免将敏感数据提交到 Git 仓库中。\n.terraform.lock.hcl 包含了版本信息，应该提交到仓库里。"},"title":"目录结构"},"/devops-learn/docs/terraform/provisioner/":{"data":{"":"某些基础设施对象需要在创建后执行特定的操作才能正式工作。\n像这样创建后执行的操作可以使用预置器(Provisioner)。预置器是由 Terraform 所提供的另一组插件，每种预置器可以在资源对象创建后执行不同类型的操作。","cloud-init#cloud-init":"不少公有云厂商的虚拟机都提供了 cloud-init 功能，可以让我们在虚拟机实例第一次启动时执行一段自定义的脚本来执行一些初始化操作。\n首先要指出的是，provisioner 的官方文档里明确指出，由于预置器内部的行为 Terraform 无法感知，无法将它执行的变更纳入到声明式的代码管理中，所以预置器应被作为最后的手段使用，那么也就是说，如果 cloud-init 能够满足我们的要求，那么我们应该优先使用 cloud-init。","user_data#user_data":"user_data 执行的命令, Terraform 无法感知. 如果命令失败,不得不访问服务器,再进行调试.","user_data-和-provisioner-区别#user_data 和 provisioner 区别":"user_data: 原理是在实例启动时,将 user_data 中的内容写入到实例的 /var/lib/cloud/instance/user-data.txt 文件中,并执行该文件中的内容.\nprovisioner: 原理是在实例启动后, Terraform 通过 SSH 连接到实例,并执行 provisioner 中的命令.","为什么不建议使用-provisioner#为什么不建议使用 provisioner":""},"title":"Provisioner"}}